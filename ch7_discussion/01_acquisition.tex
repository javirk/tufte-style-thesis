% Rethinking Data Labeling:
%   Standard data labeling strategies and fine-tuning
%   Our approach to data labeling

\section{Rethinking Data Labeling}
\label{sec:disc_fullweak}

The preparation of data represents a fundamental stage within the machine learning pipeline. It is frequently assumed that the availability of a vast quantity of data is indispensable for the successful implementation of any machine learning application\sidenote{In the end, there is this field called \textit{Big Data}, right?}. However, data quality is of even greater importance, as discussed in \yeartextcite{widner2023lessons}. On the same line, the statistician Xiao-Li Meng demonstrated in a 2016 presentation that without data quality, very large quantities of data are required to reduce bias\sideauthorcite{meng2016statistical}. There is, however, one variable that research studies often disregard: monetary budget. As we saw in \Cref{fig:supervision_prices}, high-quality labeling raises the cost of a project. The question then turns into whether we can increase the performance of a model by using lower-quality data\sidenote{In this context, lower-quality data means \textit{more weakly annotated}.} while keeping the budget constant.

We address this topic in \Cref{chapter:fullweak}, where we focus on the task of image semantic segmentation. Building a segmentation dataset is known to be time-consuming, and the procedure of producing ground-truth segmentation annotations is still a tedious, manual task. When budget enters the game, traditional machine learning practitioners rely on fully supervised fine-tuning to achieve the best possible results, often from a pretrained model in a different domain. Given the expensive annotation costs, this approach incurs a reduced amount of different information fed to the model, which can promote overfitting. This problem is accentuated in fields that require experts to annotate the data, such as medicine. High-quality, annotated data in this field is scarcer.

\subsection{Our Approach to Data Labeling}
Being aware of the previously mentioned constraints in the medical field, we hypothesize in \Cref{chapter:fullweak} whether it is possible to combine weak and full annotations and, if so, how to do it. By definition, weak annotations are cheaper than full annotations. Consequently, with a fixed budget, more images can be labeled. This seemingly simple leap provides the model with more data, which serves to reduce overfitting and enhance performance on previously unseen samples. However, determining an appropriate allocation strategy for the annotation budget is a challenging task, as there is no guarantee that the same strategy will be shared in different datasets or even models. At the same time, it could be argued that the need for weakly annotated samples decreases as the budget increases because the model can retrieve all the necessary information from fully annotated samples. This implies that the allocation strategy is not constant but should depend on the budget.

We propose a method in \Cref{chapter:fullweak} for determining the optimal budget allocation strategy. Taking the previous ideas into account, it is evident that the method must work iteratively, alternating between partial budget allocation, label acquisition from an oracle, and model training. In essence, we estimate at each iteration how different annotation choices will affect model performance and move in the direction of the Pareto optima between expected improvement and cost. Without loss of generality, we identify weak labels with image-level labels and full labels with semantic segmentation maps. With this, we implement a straightforward training schedule consisting of training first for classification and then for segmentation, in both cases with fully-supervised loss functions. 

Our method requires an initial strategy to initiate the iterative process. This involves defining a number of classification and segmentation labels according to an initial associated cost. New data is annotated and added to the available pool at each iteration. It is then sampled to fit a Gaussian Process (GP), which is used to define a surrogate function for the utility of an allocation strategy. The surrogate utility function is then employed to estimate the next best strategy for the next iteration.

We evaluate our method against fixed policies representing the budget ratio allocated to annotating for segmentation or classification. Aside from our method, the experiments confirm the previous claims: best-performing strategies differ per dataset and budget. Therefore, choosing a fixed strategy blindly is, on average, a suboptimal choice. Regarding our method, the results show that it can consistently produce good performances across different budget ratios and datasets. We attribute the positive result to the iterative nature of the method, which allows it to adapt to the budget scenario. However, we also recognize that our method has limitations. First, the GP is heavily influenced by the choice of the mean prior. Not all datasets are guaranteed to behave similarly\sidenote{\textit{I. e.}~logarithmically, which we enforced in the mean prior.} with new annotations, and this behavior is only appreciated at the end of the data labeling process. Therefore, an adaptive mean prior might be needed if this method is taken to a real scenario. Second, because of the definition of the utility function, the method is greedy in nature, meaning that its adaptation capacity is limited. It will show a tendency to follow the current best-performing strategy.

With this work, we have seen that the demand for larger annotated datasets can be counteracted by strategically allocating the available budget. For low-budget regimes, carefully allocating this budget is more profitable than blindly choosing a fixed policy. These findings are relevant to our main field of study --- medical imaging ---, where annotated samples are scarce.

