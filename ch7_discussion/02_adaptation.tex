% Adapt to unseen data:
%   Large models need less adaptation
%   Into the unsupervised

\section{Adapt to Unseen Data}\label{sec:disc_adapt}
\Cref{chapter:samda,chapter:tist} deal with the domain adaptation problem in two different ways. First, \Cref{chapter:samda} proposes a training method for large models that achieves better generalization by training only a small subset of the parameters. Then, \Cref{chapter:tist} addresses unsupervised domain adaptation by leveraging invariance to image transformations.

\subsection{Large Models Need Less Adaptation}
Training large models is a challenging engineering task. Not only do they need a lot of data, but one also has to consider the limited memory of the GPUs where they are stored. Nowadays, many large models have grown beyond a single GPU. This has sparked new research on model and data parallelism to increase the number of GPUs during training. At the same time, some efforts have been directed to mixed precision training\sideauthorcite{micikevicius2017mixed} to nearly halve the memory consumption of deep learning models during training.

In parallel, the method we propose in \Cref{chapter:samda} and other PEFT techniques try to reduce the models' trainable parameters, assuming that large models already have a sufficient representation of the world that can be adapted to niche areas that need further fine-tuning. In that chapter, we develop an adapter for the Segment Anything Model (SAM)\sideauthorcite{sam}. This adapter leverages a pretrained model that inherently contains domain knowledge. As a result, neither the image encoder nor the mask decoder require substantial parameter updates during the adaptation phase. This design choice significantly reduces the number of trainable parameters to only an overhead of 1\% of the parameters of the whole network.

In order to evaluate the efficacy of our method, we compare it against other PEFT baselines on three tasks: in-domain, fully supervised semantic segmentation; out-of-domain generalization; and test-time domain adaptation. For the out-of-domain tasks, we utilize three medical datasets comprising images captured from two distinct devices, creating two domains. This results in the creation of two domains, thereby introducing a domain shift. In the case of the in-domain task, we also add a natural image dataset. The base vision transformer in SAM is always initialized with pretrained weights, whether from larger medical datasets or the official pretraining weights. The results demonstrate that training with the adapter is superior at generalizing precisely because it leverages the domain knowledge rooted in the neural network. It also exhibits superior performance in test-time domain adaptation due to the small number of trainable parameters it encompasses. Finally, the performance gain against full fine-tuning decreases as the number of training images increases. This is an expected outcome: with more data samples, the training distribution widens. Because the capacity of the adapter is much smaller than that of the whole model, it cannot adapt to wide distributions.

These results suggest that adapting a large model to one's needs is possible. The vision transformer we used in \Cref{chapter:samda} is only the base one, with 86M parameters. We believe comparable results could be obtained with larger models using a similar adaptation approach, assuming that the quality of the pretraining weights also increases. 

\subsection{Into the Unsupervised}
\Cref{chapter:tist} addresses the adaptation problem differently. Instead of relying on a large model to extract knowledge, this chapter focuses on unsupervised domain adaptation for semantic segmentation, a problem in which no ground-truth labels are available for the target domain. As previously discussed in \Cref{sec:domain_adaptation}, this problem resembles a real-world application. In a real scenario, having semantic segmentation labels for each device and configuration would be unbelievable, especially in highly specific fields like medicine, where expertise is essential. Semi-supervised learning has sought to reduce annotation requirements in the target domain, involving methods that promote learning abstract representations from an unlabeled target set and extending decision boundaries to align with the target dataset distribution.

The proposed method employs transformation-invariant, highly-confident pixel predictions in the target dataset for self-training. This is accomplished using an ensemble of high-confidence predictions from various non-spatially transformed versions of the same input. The loss function comprises two terms: first, a regularization cross-entropy term between the source domain labels and their corresponding predictions. Additionally, a pseudo-supervised branch on the most confident pixels between the target predictions before and after transformations. The contribution of the second term is increased progressively as training progresses. The evaluation against six alternative methods for unsupervised domain adaptation across three datasets and two neural network architectures demonstrates that pseudo-label training with high-confidence pixels is superior on average to other alternatives.  

In the context of the whole thesis, \Cref{chapter:tist} demonstrates that semi-supervised learning is a powerful strategy for those scenarios where labels are unavailable. In my opinion, domain adaptation problems in which target domains are unavailable\sidenote{Unsupervised DA is one of them, but also test-time and source-free.} lies at the top of the difficulty. In these problems, the model not only has to learn meaningful representations without supervision labels but also be able to extract the base knowledge about the field that is preserved inside and discard whatever features are meaningless. 

The results in the mentioned chapter, as well as in several published works, evidence that domain adaptation without target labels can be achieved. This suggests that the data distributions of the different domains are similar. This realization alleviates the pessimistic perspective that started the present chapter: the need for labeled data might be less pressing than was delineated at the beginning of this thesis. Instead, complementary efforts should be (and are) taken toward a better comprehension of the inner workings of deep learning models. This would help us develop more capable adaptation methods.