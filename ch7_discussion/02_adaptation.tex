% Adapt to unseen data:
%   Large models need less adaptation
%   Into the unsupervised

\section{Adapt to Unseen Data}\label{sec:disc_adapt}
\Cref{chapter:samda,chapter:tist} dealt with the domain adaptation problem in two different ways. First, \Cref{chapter:samda} proposed a training method for large models that achieved better generalization by training only a small subset of the parameters. Then, \Cref{chapter:tist} addressed unsupervised domain adaptation by leveraging invariance to image transformations.

\subsection{Large Models Need Less Adaptation}
Training large models is a challenging engineering task. Not only do they need a lot of data, but one also has to consider the limited memory of the GPUs where they are stored. Nowadays, many large models have grown beyond a single GPU. This has sparked new research on model and data parallelism to increase the number of GPUs during training. At the same time, some efforts have been directed to mixed precision training\sideauthorcite{micikevicius2017mixed} to nearly halve the memory consumption of deep learning models during training.

In parallel, the method we proposed in \Cref{chapter:samda}, along with other PEFT techniques, tries to reduce the models' trainable parameters, assuming that large models already have a sufficient representation of the world that can be adapted to niche areas that need further fine-tuning. In that chapter, we developed an adapter for the Segment Anything Model (SAM)\sideauthorcite{sam}. This adapter leveraged a pretrained model that inherently contained domain knowledge. As a result, neither the image encoder nor the mask decoder required substantial parameter updates during the adaptation phase. This design choice significantly reduced the number of trainable parameters to only an overhead of 1\% of the parameters of the whole network.

In order to evaluate the efficacy of our method, we compared it against other PEFT baselines on three tasks: in-domain, fully supervised semantic segmentation, out-of-domain generalization, and test-time domain adaptation. For the out-of-domain tasks, we utilized three medical datasets comprising images captured from two distinct devices, creating two domains, thereby introducing a domain shift. In the case of the in-domain task, we also added a natural image dataset. The base vision transformer in SAM was always initialized with pretrained weights, whether from larger medical datasets or the official pretraining weights. The results demonstrated that training with the adapter was superior at generalizing precisely because it leveraged the domain knowledge rooted in the neural network. It also exhibited superior performance in test-time domain adaptation due to the small number of trainable parameters it encompassed. Finally, the performance gain against full fine-tuning decreased as the number of training images increased. This was an expected outcome: with more data samples, the training distribution widened. Because the adapter's capacity was much smaller than that of the whole model, it could not adapt to wide distributions.

These results suggest that adapting a large model to one's needs is possible. The vision transformer we used in \Cref{chapter:samda} is only the base one, with 86M parameters. We believe comparable results could be obtained with larger models using a similar adaptation approach, assuming that the quality of the pretraining weights also increases. 

\subsection{Into the Unsupervised}
\Cref{chapter:tist} addressed the adaptation problem differently. Instead of relying on a large model to extract knowledge, this chapter focused on unsupervised domain adaptation for semantic segmentation, a problem in which no ground-truth labels are available for the target domain. As previously discussed in \Cref{sec:domain_adaptation}, this problem resembles a real-world application. In a real scenario, having semantic segmentation labels for each device and configuration would be implausible, especially in highly specific fields like medicine, where expertise is essential. Semi-supervised learning has sought to reduce annotation requirements in the target domain, involving methods that promote learning abstract representations from an unlabeled target set and extending decision boundaries to align with the target dataset distribution.

The proposed method employed transformation-invariant, highly-confident pixel predictions in the target dataset for self-training. This was accomplished using an ensemble of high-confidence predictions from various non-spatially transformed versions of the same input. The loss function comprised two terms: in the first place, a regularization cross-entropy term between the source domain labels and their corresponding predictions. In the second place, it included a pseudo-supervised branch acting on the most confident pixels between the target predictions before and after transformations. The contribution of the second term was increased progressively as training progressed. The evaluation against six alternative methods for unsupervised domain adaptation across three datasets and two neural network architectures demonstrated that pseudo-label training with high-confidence pixels was superior on average to other alternatives.  

In the context of the whole thesis, \Cref{chapter:tist} demonstrated that semi-supervised learning is a powerful strategy for those scenarios where labels are not provided. In my opinion, domain adaptation problems in which target domains are unavailable\sidenote{Unsupervised DA is one of them, but also test-time and source-free.} lies at the top of the difficulty. In these problems, the model has to learn meaningful representations without supervision labels and extract the intrinsic knowledge inside the data, discarding unimportant and distracting features.

The results in the mentioned chapter, as well as in several published works cited throughout this thesis, prove that domain adaptation without target labels can be achieved. This realization alleviates the pessimistic perspective that started the present chapter: the need for labeled data might be less pressing than was delineated at the beginning of \Cref{chapter:introduction}. Instead, complementary efforts should be (and are) taken toward a better comprehension of the inner workings of deep learning models. This would help us develop more capable adaptation methods.