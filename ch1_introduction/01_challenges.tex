%Data Challenges in Computer Vision
%   Understanding the Challenge of Limited Data: talk about the evolution of data with respect to model size
%   Implications for Computer Vision Tasks: performance, generalization, data efficiency (methods that maximize the given data), robustness (look at wrong patterns)

% The bitter lesson: Breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach. http://www.incompleteideas.net/IncIdeas/BitterLesson.html
\section{Data Challenges in Artificial Intelligence and Computer Vision}\sidecitationnonum{sutton2019bitter}{Breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.}

The world is experiencing an unparalleled surge in technology usage and accessibility, driven by its constant evolution and the continuous decline in the prices of devices and services. Devices have become smaller and cheaper over time, which has made them accessible to a broader public. Furthermore, globalization and the Internet have intensified the interconnectedness of the world, thereby revolutionizing how humans access information and connect with each other. This has created new avenues for collaboration that have ultimately contributed and still contribute to the acceleration of progress.

\textfig[t]{1}{Figures/years_compute.pdf}{Computational capacity of supercomputers has doubled every 1.5 years from 1975 (not shown) to 2009, and this trend has been maintained in the last decade. Data from \yeartextcite{supercomputers2023dongarra}.}{fig:years_compute}

Simultaneously, the advancements in cost reduction and accessibility have been accompanied by a further significant enhancement in computing capabilities. Since 1971, when the first microprocessor was produced, the number of transistors in microprocessors has doubled every two years, following \autoindex{Moore's Law}\sideauthorcite{moore1998cramming}. Although this metric is not directly relevant to the end user, it has been replicated in other areas of interest to customers, such as speed and cost of computing. To put an example, the computational capacity\sidenote{Measured as the number of floating-point operations carried out per second, or FLOPS.} of supercomputers has doubled every 1.5 years from 1975 to 2009\sidecite{koomey2010implications}, as seen in \Cref{fig:years_compute}.

The field of Artificial Intelligence has not remained aloof from these advances but has greatly benefited from them. In 2005, the world witnessed the first hint of what the future could entail, when~\yeartextcite{steinkraus2005using} implemented a small artificial neural network\sidenote{I will present in \cref{sec:ml} a detailed description of neural networks.} on a \autoindex{Graphical Processing Unit} (GPU) and reported a three-times speedup over their CPU-based implementation. The work was followed by~\yeartextcite{chellapilla2006high}, which showcased the applicability of this technique in accelerating supervised convolutional networks. Until then, GPUs were specialized hardware components devised for video game rendering, a process that requires high parallelization\sidenote{To put it simply, each pixel operates independently of its neighboring pixels, so all of them can be rendered in parallel.}. The realization that artificial neural networks were formed by smaller components\sidenote{In \cref{sec:ml} we will learn that these small components are called ``neurons''.} that could be processed independently from others in the same layer connected the fields of graphics rendering and artificial intelligence, leading to said works. In the last decades, GPUs underwent a significant transformation, evolving from specialized gaming hardware to general-purpose processing units. Throughout this evolution, they have retained their most valuable feature, parallelization, while increasing computational capacity and memory constraints. This has resulted in a 1,000-fold increase in single-chip GPU inference performance over the past ten years\sideauthorcite{nvidia2023gpuperformance}.

\plainwidefig[t]{1}{Figures/model_compute.pdf}{Training computational requirements of machine learning models over time. Marker size represents the training dataset size; dashed lines are regressed only on the corresponding data in that period. Y-axis is logarithmic. Data from \yeartextcite{epoch2023pcdtrends}.}{fig:model_compute}

Artificial neural networks have profited from the above-described development and have grown in twenty years from two-layer fully connected networks in a \qty{256}{\mega\byte}-large GPU\sidecite{steinkraus2005using} to massively large models that require over \qty{3,000}{\giga\byte} of memory to be stored\sidenote{Number calculated with the most recent data for GPT-4. 1.8 trillion parameters at half-precision (FP16) require \qty{3520}{\giga\byte} of memory.}. However, such a frenetic growth in model size faces significant constraints stemming from energy consumption, monetary cost, and data availability. In the first place, the energy consumption of neural network training and inference processes poses a critical limitation, particularly as models scale up in size and complexity. Training large-scale neural networks demands substantial computational resources, leading to significant energy consumption and carbon emissions\sideauthorcite{luccioni2023estimating}. In the second place, the efficacy of these models is heavily reliant on the quality and quantity of the data on which they are trained. On the one hand, as the complexity of tasks tackled by neural networks expands, an ever-growing demand for larger datasets arises\sidecitation{hoffmann2022training}{We find that for compute-optimal training, the model size and the number of training tokens should be scaled equally.}.

On the other hand, ensuring the quality of said datasets requires extensive resources and expertise, which hampers the expansion of the datasets in question. This thesis focuses on this latter limitation and proposes solutions to address the constrained data availability.

\subsection{Limited Data into Numbers}
%\subsection{Understanding the Challenge of Limited Data}
To shed light on the discrepancy between the growth of models and datasets, we can analyze \Cref{fig:model_compute}, with data coming from \yeartextcite{sevilla2022compute}. From this, we can draw two primary conclusions: Firstly, the emergence of the first viable Deep Networks around 2010 significantly accelerated model growth, reducing the duplication rate from every 1.4 years to every six months\sidenote{From 1955 to 2010, model size increased steadily at the rate of 1.4x per year, following Moore's Law approximately. From 2010 to 2023, the rate was 4.1x per year.}. Secondly, dataset sizes previously tracked model size during the Pre Deep Learning Era, duplicating roughly every 20 months. However, they are currently lagging behind, with a duplication rate of every ten months. % This deceleration is particularly evident in datasets related to vision tasks.

\subsection{Potential Implications for Computer Vision Tasks}
% performance, generalization, data efficiency (methods that maximize the given data), robustness (look at wrong patterns) <-- this is dropped, maybe will add in the future
A model's learning capacity is directly proportional to the number of samples in the training set\sidenote{This growth is logarithmic for fully supervised training, as will be explained in \Cref{sec:fullweak_method}.}. Therefore, the most immediate consequence of limited data availability will be initially manifested as suboptimal performance on the corresponding testing set. However, we will see that the negative implications extend beyond low performance on in-distribution testing datasets.

Looking at the limited data problem from a different angle, a model may encounter difficulties extrapolating the training data when the training set fails to fully represent all aspects of a domain. This situation can result in unexpected failures when the model is deployed in production environments. In real-world settings, the model will be exposed to a multitude of scenarios that small training sets may not capture adequately, leading to a lack of generalization\sidedef{generalization}{Model's capability to adapt to unseen data, especially that coming from a different distribution.}.

Negative implications, as bad as they seem, open the field for thoughtful solutions. Generalization is a burden in small annotated datasets, but one can pose several meaningful questions: Would it be possible to increase this model's feature by using unlabeled data? Can one use the data as efficiently as possible to achieve the best result? The present thesis focuses on these types of solutions.