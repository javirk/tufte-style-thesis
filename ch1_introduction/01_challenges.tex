%Data Challenges in Computer Vision
%   Understanding the Challenge of Limited Data: talk about the evolution of data with respect to model size
%   Implications for Computer Vision Tasks: performance, generalization, data efficiency (methods that maximize the given data), robustness (look at wrong patterns)

% The bitter lesson: Breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach. http://www.incompleteideas.net/IncIdeas/BitterLesson.html
\section{Data Challenges in Artificial Intelligence and Computer Vision}\sidecitationnonum{sutton2019bitter}{[...] Breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.}

The world has experienced an unparalleled surge in technology usage and accessibility, driven by its constant evolution and the continuous decline in the prices of devices and services. Devices have become smaller and cheaper over time, which has made them accessible to the general public. Furthermore, globalization and the Internet have intensified the interconnectedness of the world, revolutionizing the manner in which humans access information and connect with each other. This has created new avenues for collaboration that have ultimately contributed and still contribute to the acceleration of progress.

\textfig[t]{1}{Figures/years_compute.pdf}{Computational capacity of supercomputers has doubled every 1.5 years from 1975 to 2009, and this trend has been maintained in the last decade. Data from TOP500 Supercomputer Database (2023) â€“ processed by Our World in Data.}{fig:years_compute}

Simultaneously, the advancements in cost reduction and accessibility have been accompanied by a further significant enhancement in computing capabilities. Since 1971, when the first microprocessor was produced, the number of transistors in microprocessors has doubled every two years, following \autoindex{Moore's Law}\sideauthorcite{moore1998cramming}. Although this metric is not directly relevant to the end user, it has been replicated in other areas that customers do care about, such as speed and cost of computing. To put an example, the computational capacity\sidenote{Measured as the number of floating-point operations carried out per second, or FLOPS.} of supercomputers has doubled every 1.5 years from 1975 to 2009\sidecite{koomey2010implications}, as seen in \Cref{fig:years_compute}.

The field of Artificial Intelligence has not remained aloof from these advances but has greatly benefited from them. In 2005, the world witnessed the first hint of what the future could entail, when~\yeartextcite{steinkraus2005using} implemented a small fully connected neural network on a \autoindex{Graphical Processing Unit} (GPU) and reported a three-times speedup over their CPU-based implementation. The work was followed by~\yeartextcite{chellapilla2006high}, which showcased the applicability of this technique in accelerating supervised convolutional networks. Up until then, GPUs were specialized hardware components devised for video game rendering, which requires high parallelization\sidenote{To put it simply, each pixel operates independently of its neighboring pixels, so all of them can be rendered in parallel.}. The realization that fully connected networks were formed by ``neurons'' that can be processed independently from others in the same layer connected both fields and led to said works. In the last decades, GPUs underwent a significant transformation, evolving from specialized gaming hardware to general-purpose processing units. Throughout this evolution, they have retained their most valuable feature, parallelization, while increasing computational capacity and memory constraints. This has resulted in a 1,000-fold increase in single-chip GPU inference performance over the past ten years\sideauthorcite{nvidia2023gpuperformance}.

\plainwidefig[t]{1}{Figures/model_compute.pdf}{Training compute of machine learning models over time. Marker size represents the training dataset size; dashed lines are regressed only on the corresponding data in that period. Y-axis is logarithmic. Data from \cite{epoch2023pcdtrends}.}{fig:model_compute}

Neural networks have profited from the above-described development and have grown in twenty years from two-layer fully connected networks in a \qty{256}{\mega\byte}-large GPU~\sidecite{steinkraus2005using} to massively large models that require over \qty{3000}{\giga\byte} of memory to be stored\sidenote{Number calculated with the most recent data for GPT-4. 1.8 trillion parameters at half-precision (FP16) require \qty{3520}{\giga\byte} of memory.}. Such a frenetic growth in model size, however, faces significant constraints stemming from both energy consumption and data availability. In the first place, the energy consumption of neural network training and inference processes poses a critical limitation, particularly as models scale up in size and complexity. Training large-scale neural networks demands substantial computational resources, leading to significant energy consumption and carbon emissions\sideauthorcite{luccioni2023estimating}. In the second place, the efficacy of these models is heavily reliant on the quality and quantity of the data on which they are trained. On the one hand, as the complexity of tasks tackled by neural networks expands, an ever-growing demand for larger datasets arises\sidecitation{hoffmann2022training}{We find that for compute-optimal training, the model size and the number of training tokens should be scaled equally.}.

On the other hand, ensuring the quality of said datasets requires extensive resources and expertise, which hampers the expansion of the datasets in question. This thesis focuses on this latter limitation and proposes solutions to address the constrained availability of data.

\subsection{Limited Data into Numbers}
%\subsection{Understanding the Challenge of Limited Data}
To shed light on the discrepancy between the growth of models and datasets, we can analyze \Cref{fig:model_compute}, with data coming from \yeartextcite{sevilla2022compute}. From this, we can draw two primary conclusions: Firstly, the emergence of the first viable Deep Networks circa 2010 significantly accelerated model growth, reducing the duplication rate from every 1.4 years to every six months\sidenote{From 1955 to 2010, model size increased steadily at the rate of 1.4x per year, following Moore's Law approximately. From 2010 to 2023, the rate was 4.1x per year.}. Secondly, dataset sizes previously tracked model size during the Pre Deep Learning Era, duplicating roughly every 20 months. However, they are currently lagging behind, with a duplication rate of every ten months. This deceleration is particularly evident in datasets related to vision tasks.

\subsection{Potential Implications for Computer Vision Tasks}
% performance, generalization, data efficiency (methods that maximize the given data), robustness (look at wrong patterns) <-- this is dropped, maybe will add in the future
The negative implications of limited data availability extend beyond low performance on in-distribution testing datasets. Indeed, the model's learning capacity increases with the number of samples in the testing set\sidenote{This growth is logarithmic for fully supervised training, as will be explained in \Cref{sec:fullweak_method}.}. Therefore the immediate and most evident consequence will be suboptimal performance on the corresponding testing set.

Additionally, looking at the limited data problem from a different angle, when the training set fails to fully represent all aspects of a domain, a model may encounter difficulties in extending beyond the training data. This situation can result in unexpected failures when the model is deployed in production environments. In real-world settings, the model will be exposed to a multitude of scenarios that small training sets may not capture adequately, leading to a lack of generalization\sidedef{generalization}{Model's capability to adapt to unseen data.}.

Negative implications, as bad as they seem, open the field for thoughtful solutions. Generalization is a burden in small annotated datasets, but we can ask a number of meaningful questions: Would it be possible to increase this model's feature by using unannotated data? Can we use the data as efficiently as possible to achieve the best result? The present thesis focuses on these types of solutions.