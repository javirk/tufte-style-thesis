% Overcoming Budget Constraints
%   Weak annotations
%   Adaptive Annotation strategies
%   Active Learning: Not sure, it can be a very short part
\section{Overcoming Budget Constraints}
Nothing is free, and this holds especially true in the field of machine learning. The formidable advancements that we have seen in computing power and model size (\Cref{fig:years_compute,fig:model_compute}) have led to a simultaneous increase in the required operational resources. As models become more complex, the resources they require scale accordingly---more powerful hardware, greater data storage, longer training time, and increased energy consumption. These escalating demands are having a direct influence on the costs incurred, both in financial terms and in terms of the environmental impact\sidenote{At the writing of this text, Meta has launched Llama 3, whose training has emitted \qty{2,290}{\tonne} of CO2 equivalent~\parencite{llama3modelcard}. For reference, a flight from Madrid to New York costs around \qty{0.9}{\tonne} per passenger.}. Moreover, the necessity for larger datasets, as previously discussed, is fueling the demand for human annotators who label all the data. These annotators are often employed in suboptimal conditions\sideauthorcite{time2023chatgpt} to counteract the rising costs.

While this may appear to be a problem in the long term, it is imperative to take action today in order to be prepared for what will inevitably occur. Even medium-sized companies will eventually be unable to train models from scratch, and they will have to rely on pretrained models. The problem will be exacerbated by the fact that a very small number of large corporations will have the capacity to control these pretrained models\sidenote{I believe only Open-Source alternatives will be able to counteract this trend that is already emerging.}. This will result in the unavoidable admission of the inherent biases these models will entail due to the limited number of people who will supervise their training. Consequently, research into methods of reducing costs at any stage of the machine learning pipeline\sidedef{Machine Learning Pipeline}{Workflow of a machine learning model encompassing all the required steps to put it into production. From data acquisition and labeling to deployment.} will be beneficial in the future.

\textfig[t]{1}{Figures/weakannotations.pdf}{Four different kinds of annotations. Namely, point, bounding boxes, polygons (also called coarse segmentation), and fine-grained segmentation. The images and segmentation annotations have been taken from the Cityscapes dataset~\parencite{cordts2016cityscapes} and cropped for better visualization. Faces have been blurred.}{fig:weakannotations}

Several methods have appeared to leverage data and annotations in this context, focusing on balancing costs and compelling practitioners to rethink how they approach machine learning development. The present section explores this question, discussing the role of less informative types of annotations and adaptive labeling strategies.

\subsection{Weak Annotations: Annotate Quickly, Analyze Carefully}\index{weak annotations}
Consider a scenario in which you are tasked with annotating many images for a computer vision project. Would you prefer to label each image with a single word, draw a box around each object, or use accurate polygons? Given the tedious nature of the annotating task, you would likely choose the one that requires the least time. Intuitively, the process of labeling for image segmentation is more time-consuming than that of image classification because one must classify each pixel semantically (as seen in \Cref{fig:weakannotations}). Formally, one can define the amount of information conveyed by an annotation $X$ using Shannon's formula:
\begin{equation}
    H(X)=-\sum_{x\in X} p(x)\log_2 p(x).
\end{equation}

In the context of image classification, this implies that a label in a balanced dataset with ten classes in a uni-label setting conveys approximately \qty{3}{\bit} of information. However, this figure is multiplied by the number of pixels for image segmentation, resulting in about \qty{33}{\kilo\bit} for a $100\times 100$ image.\sidenote{For object detection, this number lies in the middle of both and depends on the number of objects. It is approximately \qty{282}{\bit} for five objects. This calculation is slightly more involved; for further details, refer to \Cref{app:bits}.}

From a more practical perspective, \Cref{fig:supervision_prices} shows the market price and estimated labeling time for different types of annotations. It demonstrates that segmentation is significantly more costly than classification in terms of time and money. Furthermore, it corroborates the previous idea that the greater the amount of information to be annotated, the greater the cost of the labeling work. For a similar reason, labeling cost is further increased as soon as the domain requires expert annotators with great field knowledge. This is the case of medical annotations.

\textfig[t]{1}{Figures/supervision_prices.pdf}{Annotation time and price for different types of labels. The more information a type of annotation entails, the longer the labeling time and cost. Time data retrieved from \yeartextcite{Bearman16}, monetary costs from \yeartextcite{googlecompute}.}{fig:supervision_prices}

% Weak annotations are a form of data labeling where the provided information is less detailed or coarse in nature compared to traditional, fine-grained annotations. Unlike full\sidenote{Or strong.} annotations, which give explicit, pixel-level, or detailed region boundaries; weak annotations offer only basic cues about an image's content.  

Returning to the initial question, it appears that image-level annotations, such as classification, would be the preferred option due to the reduced amount of work involved. However, this would result in segmentation tasks, which are highly labeling-greedy, being disregarded in traditional settings that require full-level supervision. In contrast, weak annotations\sidedef{weak annotations}{Data labels that provide coarse or general information about a data point. They are less complex than the final task and are therefore designated as ``weak''.} differ from traditional supervision in the amount of information provided, offering a broader or less granular level of labeling. They provide sufficient information to facilitate segmentation without the expense and labor involved in traditional strong annotations, although the training method must be modified. In this respect, image-level labels, bounding boxes, points, or scribbles are considered weak annotations for the segmentation task, as the latter requires more information.

The increasing availability of weak annotations is providing a powerful alternative to full annotations, particularly in the field of computer vision. In recent times, text annotations and image descriptions without specific labels have emerged as a valuable resource for model pretraining in frameworks such as CLIP\sideauthorcite{radford2021learning}. 

\subsection{Adaptive Annotation Strategies and Active Learning}\index{adaptive annotation}\index{active learning}
Effective annotation is key to building robust models. As previously discussed, weak annotations entail less precise or partially labeled data, which can be beneficial for model training but only when used effectively. The introduction of these annotations has opened the door to alternative training methods. However, the challenge lies in maximizing their utility without compromising model accuracy. 

Adaptive annotation strategies and active learning have proven to be effective approaches to address this challenge. On the one hand, adaptive annotation involves adjusting the annotation process dynamically to focus on the most informative annotation type or data point. Such flexibility allows for the efficient use of weak annotations by concentrating resources where they can have the most significant impact. This is typically achieved via a policy that acts iteratively and evaluates the most informative annotation strategy for the model at each iteration. A more detailed examination of this topic is presented in \Cref{chapter:fullweak} of this work, in which one such policy is also proposed.

For its part, active learning\sidedef{active learning}{Learning paradigm in which the algorithm iteratively queries the user to label new data points based on their potential learning benefit.} is a type of adaptive annotation strategy that focuses on data sampling. In this case, the annotation type generally remains unchanged, and the policy's objective is to select the data points that will facilitate the greatest improvement in the model's performance in the next iteration. Consequently, human annotator efforts are directed toward the most informative samples, thereby reducing the overall annotation workload while maintaining model robustness.

