%Overview of Thesis Structure
\section{Organization}
\Cref{chapter:background} provides the technical background that informs the rest of the dissertation. It begins with an introduction to the fundamentals of machine learning and deep neural networks in \Cref{sec:ml} for vision tasks with a special emphasis on the mechanism of attention layers (\Cref{sec:attention}), their integration into Vision Transformers, and the implications of this on model training evading end-to-end fine-tuning. It then shifts to other machine learning topics, including Gaussian Processes (\cref{sec:gaussian_process_background}), the presentation of non-traditional training strategies such as semi-supervised training in \Cref{sec:training_paradigms}, and several domain adaptation flavors in \Cref{sec:domain_adaptation}. The final section \cref{sec:ophthalmology} explores concepts in ophthalmologic image acquisition that will be needed throughout the dissertation.

\Cref{chapter:fullweak} manages the earliest task in the machine learning pipeline, namely the data acquisition process from the cost allocation perspective. This chapter explores the possibility of mixing different kinds of annotations by defining a strategy that aims to maximize performance with budget constraints. It puts the concept of Gaussian Processes into use to extrapolate model performance with unseen data in an iterative manner in \Cref{sec:fullweak_method}. Subsequently, it is demonstrated that an adaptive strategy can achieve near-optimal performance for a range of annotation budgets and datasets.

\Cref{chapter:samda} tackles the domain adaptation problem. As previously discussed, end-to-end fine-tuning of models is turning computationally intractable. This chapter introduces a novel adapter for one of the most well-known semantic segmentation architectures that minimizes the number of trainable parameters while maintaining the performance of the base model. 

\Cref{chapter:tist} further develops the domain adaptation problem for image segmentation from the unsupervised point of view. Specifically, it addresses the problem of overcoming distribution shifts between different imaging devices and configurations in the clinical setting. It does so by employing a self-training technique that leverages the fact that neural network performance should be invariant to image transformations (\cref{sec:tist_methodology}). The proposed method, designated as TI-ST\sidenote{Acronym for Transformation-Invariant Self-Training}, is subjected to evaluation in three medical image modalities in comparison with several state-of-the-art domain adaptation methods (\cref{sec:experimental_results_tist}).

\Cref{chapter:oct} focuses on the medical aspects of data usage and addresses one of the final stages of the machine learning pipeline, namely the application of a model in a practical setting. The chapter proposes a method for locating biological markers from Optical Coherence Tomographies that only requires weak annotations. It also portrays how to leverage domain knowledge to formulate a loss function that constrains the output \cref{sec:oct_method}. Finally, the presented method is tested on a variety of settings \cref{sec:oct_results}.

\Cref{chapter:discussion} presents a summary of the contributions made by this thesis and a discussion of the findings and shortcomings. Finally, \Cref{chapter:conclusion,chapter:future_work} provide a conclusion  and an outlook on future work, respectively.