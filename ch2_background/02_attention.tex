% Attention Mechanisms and Vision Transformers
%   Understanding Attention in Neural Networks
%   Vision Transformers (ViT): An Emerging Paradigm -> say also that they are not much better than convnets for medical things
%   Prompts for Vision Transformers - look at ECCV related work section

% Motivation: 
% feedforward networks and convnets: contribution of a value driven by its location.
% Attention: dynamically identify the parts of the signal that are more relevant --> translation.
%   Combine information from far away parts of the image. Discard information.
%   Features aggregated with an importance score that depends on the features themselves

\section{Attention Mechanisms and Vision Transformers}
\label{sec:attention}
Attention-based networks appeared in 2017\sideauthorcite{vaswani2017attention} and have come to stay. The Transformer, an architecture derived from the attention-based network, has become a widely used term\sidenote{GPT, the model under ChatGPT, stands for Generative Pre-trained Transformer.} in the context of a general public product utilized by 100 million users on a weekly basis\sideauthorcite{verge2023chatgpt}. At the time of writing, transformers are positioned as a near-term alternative to traditional neural networks, mainly due to their ability to scale efficiently, allowing training with millions of data points. All these advancements have perfused into computer vision, with more capable than ever models for image and video generation\sideauthorcite{blattmann2023stable} and scene understanding. Before diving into how this progress is shaping the future of artificial intelligence, we will discuss the motivation for a new kind of operation, different from the fully connected layers or convolutions we saw in the previous sections.

One of the main differences between a convolutional layer with respect to a fully connected layer is the receptive field\sidedef{Receptive field}{Region in the input space that affects a particular feature in the output space.}. In the former, each feature is affected by the features nearby, whereas in the latter, each unit receives information from the whole feature tensor. In both cases, however, the influence of an input value on the output depends entirely on their relative positions\sideauthorcite{attention_slides}. There exist tasks for which this arrangement is suboptimal: for instance, languages do not all share the same grammatical structures, and this has to be taken into account during translation\sidenote{To put an example, in German, the verb always comes in the second place, whereas in Japanese it is always at the end of the sentence.}. In the same context, words written at the beginning of a sentence can influence others far away. The lack of a processing method that dynamically identified the relevant structures for a given input motivated the development of attention-based models.

\subsection{Understanding the Attention Mechanism and its Implications}
The fundamental characteristic of the attention mechanism is the dynamic focus on different parts of the input. We will see now how this is achieved for 1-D sequences of length $T$ with $D$-dimensional elements in Scaled Dot-Product Attention as explained in \yeartextcite{vaswani2017attention} and shown in \Cref{fig:attention_diagram}. In the next section, we will generalize this to two-dimensional sequences (\ie~images).

Attention starts with three token\sidedef{token}{Elements into which a sequence can be divided.} sequences: queries ($Q\in \real^{M\times D_k}$), keys ($K\in \real^{N\times D_k}$) and values ($V\in \real^{N\times D_v}$)\sidenote{These names are not whimsical. Refer to \yeartextcite{graves2014neural} for historical context.}. The two first are combined to compute an attention matrix $A\in \real^{M\times N}$ which is weighted by $V$ to get the result sequence $Y\in \real^{M\times D_v}$:

\begin{equation}
    Y = \text{Attention}(Q, K, V) = \underbrace{\softmax\left(\dfrac{QK^T}{\sqrt{D}}\right)}_{A \in \real^{M\times N}}V
    \label{eq:main_attention}
\end{equation}

\textfig[t]{1}{Figures/attention_diagram.pdf}{Scaled dot-product attention diagram as introduced in \yeartextcite{vaswani2017attention}. Diagram adapted from \yeartextcite{abbott2024neural}.}{fig:attention_diagram}

Attention layers implement these operations with a previous step that linearly projects the input vectors into $Q$, $K$, and $V$\sidenote{$\linear$ refers to applying a fully connected layer without activation function.}:
\begin{eqnarray*}
    Q & = & \linear^q(X) \in \real^{M\times D_k}, \\
    K & = & \linear^k(Y) \in \real^{N\times D_k}, \\
    V & = & \linear^v(Z) \in \real^{N\times D_v}.
\end{eqnarray*}
Notably, sometimes $X\equiv Y\equiv Z$. In that case, this process is called \textit{self-attention}. In \Cref{chapter:samda}, we employ an attention layer in which $Y\equiv Z$. This is known as \textit{cross-attention}.

This kind of layer gave birth to the Transformer also in \yeartextcite{vaswani2017attention}. In that work, several attention layers were computed in parallel to form \textit{multi-head self-attention} and stacked in an encoder-decoder architecture\sidenote{In addition to attention, another noteworthy contribution of that work was the use of positional encoding, which ensured that the network received information about the relative or absolute position of the tokens in the sequence.}. This was tested on sequence-to-sequence translation, achieving state-of-the-art results at a fraction of the training cost. The basic architecture of the transformer encoder is illustrated on the right-hand side of \Cref{fig:vit_architecture}.

\subsection{Vision Transformers (ViT): An Emerging Paradigm}\label{subsec:vision_transformer}
The initial formulation of transformers was designed for sequential data and, therefore, applied mainly to natural language processing (NLP). Researchers then began to explore the potential of transformers beyond text, considering their application to other types of data such as images. Nevertheless, this evolution proved more challenging than expected. Naively, one could flatten an image into a sequence of pixels and then apply attention layers to it. However, the storage requirements of this approach grow quadratically with the size of the images. As seen in \Cref{eq:main_attention}, the attention matrix $A$ dimensions depend on the sequence lengths of the queries and the keys. Some initial works attempted to replace convolutions entirely with specialized attention patterns\sideauthorcite{ramachandran2019stand}, but these approaches did not scale effectively.

\textfig[t]{1}{Figures/vit_architecture.pdf}{Vision Transformer (ViT) architecture (left) treats an image as a sequence of smaller patches rather than individual pixels. These patches are linearly projected before being fed to the transformer encoder (right). Diagram from \yeartextcite{koleshnikov2021}.}{fig:vit_architecture}

Three years had to pass until the research community invented a successful interface of this kind of architecture to computer vision. \yeartextcite{koleshnikov2021} treat an image as a sequence of smaller patches rather than individual pixels. These patches are linearly projected before being fed to the transformer encoder, which is otherwise kept untouched with respect to the original one. This slight modification in data processing allows the transformer to handle the image more efficiently. It gave rise to a family of models now considered standard, as ResNet\sideauthorcite{ResNet} is for convolutional architectures: the Vision Transformers (ViT). The left-hand side of \Cref{fig:vit_architecture} illustrates the basic model architecture of Vision Transformers. Rapidly, diverse modifications and evolutions of this architecture permeated into the traditional tasks of computer vision. Especially significant to the present thesis is the \autoindex{Segment Anything Model} (SAM)\sideauthorcite{sam}, which appended a decoder to the vision transformer to produce a promptable\sidedef{Prompting}{Providing specific input or queries to guide the model's output.} segmentation model. 

The advent of vision transformers did not necessarily result in enhanced performance out of the box\sideauthorcite{isensee2024nnu}. However, the highly efficient scalability of transformers paved the way for larger models. As previously discussed in \Cref{chapter:introduction}, scalability in deep learning is a double-edged sword: it can lead to remarkable generalization, but that comes at the expense of larger datasets and increased training costs. To illustrate, the largest models in \yeartextcite{koleshnikov2021}, with 632M parameters, demonstrated superior performance to their smaller counterparts (86M parameters) only when trained on JFT-300M, a proprietary dataset comprising 300M images\sidecitation{koleshnikov2021}{When pre-trained on the smallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models [...] Only with JFT-300M, do we see the full benefit of larger models.}. Similarly, a comparable pattern was observed for convolutional models, which outperformed ViT on ImageNet but not on larger datasets.

This realization takes us back to the bitter lesson from \Cref{chapter:introduction}, whereby scalability, compute power, and data outperform a human-centric approach. At the same time, it brings additional difficulty in training models with such big datasets. Researchers have tackled these limitations in the medical field by fully fine-tuning models end-to-end specifically for medical data. However, while this method offers a short-term solution, the continuous growth of models and the limited availability of medical data pose ongoing challenges. As a result, end-to-end training of future models may become feasible only for organizations with substantial resources.

\subsection{Prompting as a Novel Way of Training}\index{prompting}

In response to the challenges above, Parameter Efficient Fine-Tuning (PEFT) methods have appeared. These techniques reduce the number of trainable parameters while maintaining performance comparable to full fine-tuning. By updating only a subset of parameters, PEFT methods preserve the base model's knowledge during training on secondary tasks, mitigating issues like catastrophic forgetting and overfitting, especially when handling smaller target datasets. Among all the already extensive literature on the subject\sideauthorcite{xu2023parameter}, the method LoRA\sideauthorcite{hu2022lora} has proved robust over time. LoRA introduces two trainable low-rank matrices ($A$ and $B$) for weight updates and freezes the initial weights of attention layers $W_0$ during the update. The forward pass then results in the following:
\begin{equation*}
    h = W_0x + \Delta Wx = W_0x + BAx,
\end{equation*}
where $B\in\real^{d\times r}$ and $A\in\real^{r\times k}$ are the adaptation matrices with rank $r \ll min(d,k)$. 

This method is applied to the query, key, and value matrices within the attention layers in a transformer. Indeed, training the adaptation matrices reduces the expressiveness of the model but also increases the training efficiency due to the reduced number of parameters\sidenote{$W_0$ has $d\times k$ parameters, while $A$ and $B$ combined have $r(d+k)$.}.

Adaptation methods functioning as external components in large models were first introduced in the Natural Language Processing domain alongside large-scale models\sideauthorcite{houlsby2019parameter} with the Adapter framework, and have since expanded to other areas of deep learning. The core idea behind these methods is to insert a small-parameter module into the base model, updating only this module while keeping the pre-trained model frozen.

Some methods utilize the tokenization process of Large Language Models (LLMs) to implement PEFT through prompt tuning\sidecite{lester2021power}. A notable example is LLaMA-Adapter\sidecite{llama_adapter}, specifically designed to fine-tune LLaMA\sidecite{llama} into an instruction-following model. In particular, trainable, zero-initialized adaptation prompts are appended as a prefix to the input instruction tokens in the higher transformer layers of LLaMA. In \Cref{chapter:samda}, we will present an adapter for SAM based on LLaMA-Adapter, which shows superior performance in domain generalization.

\sectionlinenew

Before exploring different learning strategies, we will redirect our attention from the latest trends in deep learning to a more traditional machine learning algorithm: Gaussian Processes. Although this might seem out of place, introducing this algorithm will be crucial for understanding \Cref{chapter:fullweak}, which will center around it.
