% Domain Adaptation in Computer Vision
%   Why DA is important --> Start with an example in medical imaging, add a picture to show everything
%   The amount of information determines the flavor: Types of Domain Adaptation (Unsupervised, source free...)
%   Domain Adaptation in Real-World Applications  ---> Probably not!

\textfig[t]{1}{Figures/domain_adaptation_example.pdf}{Brain MRI scans from three sites (UMC Utrecht, NUHS Singapore, and VU Amsterdam) with the FLAIR acquisition. Although all examples have been acquired with FLAIR acquisition, each site has different acquisition parameters, leading to visible differences in image properties. Images from the WMH dataset~\cite{wmh}.}{fig:domain_adaptation_example}

\section{Domain Adaptation}\label{sec:domain_adaptation}\index{domain adaptation}


\Cref{fig:domain_adaptation_example} shows brain MRI scans from three sites with the same acquisition type. At the same time, \Cref{fig:domain_adaptation_intensity} presents a quantification of the pixel intensity distribution per site normalized within that site. Both figures suggest that image values are not only dependent on the field of application\sidenote{Of course, the pixel distribution of road images acquired for self-driving cars and that of brain MRIs will be completely different.}, nor on the acquisition type, but also on the particular settings defined on-site. This poses a critical problem for computer vision: how to build models that bridge the domain shift\sidenote{See \Cref{subsec:da_intro}, where this term was introduced.}, \ie~that generalize data with the same semantic distribution but different features. For example, in \Cref{fig:domain_adaptation_example}, all three images have the same semantic information (brain MRIs with white matter hyperintensities), but the images' features are very different. We say in this case that the source domain\sidedef{Source Domain}{Initial domain in which a ML model is trained.} --- the initial domain on which the model is trained --- is different from the target domain\sidedef{Target Domain}{Domain to which a pre-trained model is applied.} --- the domain to which we are applying the model.

\textfig[t]{1}{Figures/domain_adaptation_intensity.pdf}{Quantification of pixel intensity differences across three sites of the WMH dataset~\cite{wmh}.}{fig:domain_adaptation_intensity}

In this section, we will explore how the field of Domain Adaptation (DA) is searching for the answer to the previous question. We will see that the answer depends on the amount of information in both the source and the target domains during training and how the complexity of the problem increases as this variable decreases.

\subsection{The Amount of Information Determines the Flavor}
Just as the presence or absence of labels categorizes the learning paradigms, it also categorizes the different types of domain adaptation. However, the presence of two domains introduces a certain degree of complexity. For instance, one may have no source domain images available, yet a few target labels and a substantial number of target images. When developing a domain adaptation model, it becomes crucial to ascertain the scenario at hand, as the techniques employed will vary. Nevertheless, all techniques and domain adaptation paradigms aim to construct a domain invariant representation or utilize information from the source domain in the target. 

\input{Figures/da_cube}

\Cref{fig:da_cube} depicts a conceptual model of the learning paradigms, wherein the number of unlabeled source images and labeled and unlabeled target images serve as the axes\sidenote{A representation of all the learning paradigms requires one more dimension. I had trouble representing four dimensions in a 2D paper, so I had to discard one of them.}. It can be observed that not having target labels is common, as it otherwise becomes either semi-supervised or supervised learning. Additionally, there are two scenarios that only differ in the number of target images (source-free and test-time domain adaptation) and one where no target images are available (self-supervised learning).

\begin{itemize}
    \item \textbf{Semi-Supervised DA} is very similar to traditional semi-supervised learning\sidenote{See \nameref{subsec:semi_self} in \Cref{sec:training_paradigms}.}. What defines this category is the presence of some labeled target images and a larger amount of unlabeled ones. Training strategies do not differ significantly from those used for semi-supervised learning, using the unlabeled samples for feature alignment and a regularization term that leverages the labeled samples.
    
    \item In \textbf{Unsupervised DA}, target labels are not present, and source images are often available with or without labels. This implies that the model can only learn the target domain features from feature alignment tasks, and regularization can be done with the source images.
    
    \item \textbf{Source-Free DA} (SFDA) is a more challenging task compared to the previous two. In this setting, the model retains information about the source domain internally, but source labels and images are unavailable. This scenario is highly relevant for real-world applications where a pre-trained model is provided to a user who must fine-tune it on their unlabeled target dataset without sacrificing generalization.
    
    \item If in SFDA the model is given a whole unlabeled target dataset, \textbf{Test-Time DA} (TTDA) algorithms receive a single unlabeled sample, hence the name ``test-time''\sidenote{This is also called ``online'' training. Any other scenario with a given training set is known as ``offline'' training.}. The goal is to adapt the model parameters dynamically during the inference phase. TTDA methods often involve some kind of entropy minimization\sideauthorcite{wang2021tent} or self-training with pseudo-labels. The model is trained on each sample for a number of iterations. Therefore, each sample is evaluated using a different model, one that is tailored to the specific sample.
\end{itemize}

In \Cref{chapter:tist}, we propose a method for Unsupervised domain adaptation and evaluate it on three medical datasets. In \Cref{chapter:samda}, we use a simple test-time domain adaptation algorithm to evaluate the performance of a segmentation adapter on this task. Being the adapter a small trainable module inside a larger model --- and hence its capacity very limited --- we argue that it is especially suitable for tasks where the training signal is noisy, and therefore perfect for TTDA.