\section{Methods}
As depicted in \Cref{fig:circle_workflow}, we use predictions of LineaMapper on Galileo SSI data (\Cref{sec:dataset}), assemble its output with a stitching algorithm and manually revise the resulting map. This results in new training data that is used for an improved version of LineaMapper.

\subsection{Dataset}\label{sec:dataset}
The basis for the revised map are the RegMaps returned by the Galileo SSI experiment\sidecite{Belton1992}. For mapping, the photogrammetrically adjusted mosaics by\yeartextcite{Bland2021} are used. For predictions, we use individual, orthographically projected Galileo SSI images\sidecite{PDS_SSI} processed with the standard isis\sidecite{isis} pipeline for Galileo raw SSI products (\textit{gllssi2isis}, \textit{spiceinit}, \textit{gllssical}, \textit{trim}, \textit{noisefilter} with a 3x3 kernel, \textit{cam2map}). The leading and trailing RegMaps consist of 64 and 85 individual SSI clock IDs, respectively. The observation IDs of the leading RegMap are: \textit{15ESREGMAP02, 17ESREGMAP02, 17ESREGMAP03}; the observation IDs of the trailing RegMap are: \textit{19ESNORLAT01, 17ESAGENOR01, 15ESREGMAP01, 17ESREGMAP01, 19ESREGMAP01, 17ESNERTRM01}.


The illumination conditions of the mosaic `17ESREGMAP02', which serves as the base for the revised map\sidenote{See \Cref{fig:RegMaps_basemap}.}, should not be neglected for interpretation and a possible training bias. The sun azimuth ranges from 270 to 284$^{\circ}$ (clockwise from north), the incidence angle from 76 to 87$^{\circ}$, the emission angle from 8.7 to 61$^{\circ}$, and the phase angle is 91.7 to 92.5$^{\circ}$. The subsolar-latitude and longitude are 2$^{\circ}$N and 155$^{\circ}$W. 


\subsection{Mapping guidance with LineaMapper v1.0}\label{sec:mapping_guidance}
% guideline and LineaMapper v1.0
We use lineament predictions generated with LineaMapper v1.0\sidecite{Haslebacher2024a} to guide the production of an exhaustive lineament map of the southern leading hemisphere part of the RegMaps. Since individual class scores can be chosen when retrieving predictions of LineaMapper, precision can be traded off with recall. For this reason, we selected the following score thresholds: 0.4 for bands, 0.25 for double ridge, 0.5 for ridge complexes, and 0.6 for undifferentiated lineae. These values ensure a precision $\geq 30\%$ and a recall $\geq 20\%$.

The predictions by LineaMapper are output separately on 224x224 image tiles, which means that mosaics of size larger than 224x224~px do not result in a continuous lineament prediction. Therefore, as a new extension, we developed an algorithm that stitches the separate predictions together to the size of an arbitrary input mosaic (Sec. \ref{sec:assembling_preds}). 
Guided by LineaMapperâ€™s assembled predictions, we map the southern, sub-Jovian, leading hemisphere contained in Galileo observation 17ESREGMAP02\sidenote{See red box in \Cref{fig:RegMaps_basemap}.}. We review each predicted lineament, re-classify and re-map where necessary. We follow the guideline for objective mapping in\sidecite{Haslebacher2024a} (Fig. A.14), except that we neglect the length, width and contrast constraints. We neglect these constraints because LineaMapper v1.0 did not learn to respect these limits and because the constraints exclude long but narrow and wide but short lineaments.

It took us a total of 32 hours to make the output of LineaMapper v1.0 on the southern leading hemisphere clean and complete (review and discussion among co-authors excluded). To break this down, we mapped 2140 features at a speed of 1.1 features per minute, which is 10\% faster than without any guiding\sidecite{Haslebacher2024a}. This time savings is expected to increase with future versions of LineaMapper. The revised map covers 1.3\% of Europa's surface at 215~m/px. LineaMapper-guided mapping reduces the mapping time expected for all of Europa's surface from 500\sidecite{Haslebacher2024a} to 308 8-hour working days. 
With the manually revised map, we train and test new versions of LineaMapper, which use the region shown in \Cref{fig:full_map}.C for training, validation, and testing. For validation (hyperparameter tuning) and testing (reporting results), 6\% of all available tiles are spared for each. LineaMapper v1.0, v1.1, and v2.0 can be compared on this test region as well as on the test set of \yeartextcite{Haslebacher2024a}, as \Cref{tab:Model_overviwe} illustrates.

Some ambiguous cases were difficult to classify as band, ridge complex, double ridge, or undifferentiated linea. For the sake of fine-tuning LineaMapper, we had to assign a classification, i.e. LineaMapper needs a training label. In part due to these hard cases, we do not expect LineaMapper v1.1 and v2.0 to classify lineament units unambiguously. The advantage of LineaMapper is, however, that it can predict overlapping masks of the same lineament, bearing different categories. 

\subsection{LineaMapper v1.1}\label{sec:LMv1.1}

To train LineaMapper v1.1\sidenote{From now on, we will denote the model names based on \Cref{tab:Model_overviwe}.} we use the same architecture as used by \yeartextcite{Haslebacher2024a}, the Mask R-CNN\sidecite{He2018}, but fine-tune on additional data tiled up to 112x112 pixel tiles. This means that we use LineaMapper v1.0 as the base and fine-tune (`re-train') it with new training data.

We train for 120 epochs with a learning rate of 0.01, which decreases after each 40 epochs by a factor of 10, and use a batch size of 6 tiles, minimum and maximum size of 200 and 300 px, respectively. Before training, boxes with an area smaller than 20 px are deleted. We also feed in tiles without any mapped lineaments to enrich the variety of surface areas seen by the model. Data augmentation is applied following \yeartextcite{Haslebacher2024a}.

We suggest that the usability of LineaMapper can be enhanced through the combination with a new, transformer-based model architecture for segmenting lineaments inside bounding boxes detected by Mask R-CNN. This would also allow for user-specific bounding box prompting.

\begin{table*}[t]
    \centering
    \caption{Overview of the different versions of LineaMapper. LineaMapper v1.1 produces bounding boxes that are input to v2.0.}
    \label{tab:Model_overviwe}
    \begin{tabular}{@{}p{2.5cm}P{2cm}p{3cm}lp{4cm}@{}}
    \toprule
    \textbf{Name} & \textbf{Short Name} & \textbf{Architecture} & \textbf{Training Set} & \textbf{Specialty} \\ \midrule
    LineaMapper v1.0 & LM1.0 & Mask R-CNN & \cite{Haslebacher2024a} & 180 - 600 m/px Galileo data (224 px tiles) \\
    LineaMapper v1.1 & LM1.1 & Mask R-CNN & \Cref{fig:full_map} & Galileo RegMaps at 215~m/px (112 px tiles) \\
    LineaMapper v2.0 & LM2.0 & Mask R-CNN + adapted SAM with ViT-B & same as LM1.1 & same as LM1.1 \\
    SAM GR box prompting & SAM-GR & adapted SAM with ViT-B & same as LM1.1 & Galileo RegMaps (112 px) with manually drawn bounding boxes \\
    SAM point prompting & SAM-pp & adapted SAM with ViT-B & same as LM1.1 & same as LM1.1 \\
    rcnn-SAM ViT-T & rcnn-SAM-t & Mask R-CNN + adapted SAM with ViT-T & same as LM1.1 & same as LM1.1 \\
    SAM ViT-T point prompting & SAM-pp-t & adapted SAM with ViT-T & same as LM1.1 & same as LM1.1 \\ \bottomrule
    \end{tabular}
\end{table*}

\subsection{LineaMapper v2.0}\label{sec:LMv2.0}
Released in 2023, the Segment Anything Model (SAM) by Meta is trained iteratively on over 1 billion masks contained in 11 million individual images, which is released as the SA-1B dataset\sidecite{Kirillov2023}. This makes SAM the most advanced foundation model to date for instance segmentation, which is the segmentation/mapping of multiple \textit{instances} of objects in an image. While Mask R-CNN is based on convolution, SAM is transformer-based and uses a pre-trained Vision Transformer\sidecite{Dosovitskiy2020} for image encoding, which embeds the image in a way that users can prompt the network to produce segmentation masks. The prompts that are fed into the mask decoder can be masks, a grid of points, boxes, or text. Since the decoder is designed for efficiency, real-time interactions are possible after the image has gone through the image encoder. 
The confidence score output by SAM equals an estimation of the Intersection-over-Union (IoU, see\sidecite{Haslebacher2024a} for a visualisation).
We feed the bounding boxes predicted by LineaMapper v1.1 into SAM to obtain better masks. We call this combination of models LineaMapper v2.0. Therefore, the boxes of v1.1 and v2.0 are equal, while the mask prediction of v2.0 comes from SAM. This reduces the functionality of the Mask R-CNN basically to the Faster R-CNN\sidecite{Ren2015}, the stand-alone object detection part of the Mask R-CNN. 

For further comparison and searching for the best model, we also test slightly modified versions of SAM, summarised in \Cref{tab:Model_overviwe}. Instead of prompting SAM with bounding boxes from Mask R-CNN, we can prompt SAM with manually drawn rectangular boxes, and for the test set, with ground truth boxes (SAM-GR). Furthermore, SAM can also be prompted with a grid of points (SAM-pp) instead of any boxes. The default encoder of SAM in LineaMapper 2.0 is the ViT-B model, where $B$ stands for \textit{Base}. This encoder has 86M parameters. We also test ViT-T, where $T$ stands for \textit{tiny}, referring to the lower amount of parameters (6M). The encoder architecture does not modify the structure of the decoder, which is shared and has nearly 4M parameters. The ViT-T SAM model is once prompted with Mask R-CNN boxes (rcnn-SAM-t), and once used as a stand-alone segmentation model with point prompting (SAM-pp-t).

To ensure comparability, SAM was fine-tuned on the same dataset and using the same data augmentations as LineaMapper v1.1. However, due to the different training requirements, SAM was not presented with the images without any objects inside. Additionally, as the architectures of LineaMapper v1.1 and SAM differ, so do their pretraining weights. LineaMapper v1.1 shares the same model as LineaMapper v1.0, which serves as a pretraining for the former. In contrast, SAM uses the weights from the SA-1B dataset.

The recently published Segment Anything model by Meta (SAM) provides a powerful semantic segmentation tool. Since SAM is promptable by masks or boxes, the output of LineaMapper can be used directly as a prompt input to SAM. The performance of SAM on the same generic test set is higher than the performance of Mask R-CNN\sidecite{He2018, Kirillov2023}. This result motivated us to develop LineaMapper v2.0, expecting a higher performance on the lineament detection task.

We adapt SAM to a stand-alone instance segmentation tool by substituting its default three-layer mask output with class layers (one mask per class/category). Because the original SAM is not prepared for instance and semantic segmentation altogether, we split the training process into two stages where we train until convergence. First, we train for semantic segmentation (i.e. without any kind of prompting). Then, we fine-tune the pre-trained model with prompting information. In both cases, we train with a learning rate of $10^{-4}$ and apply the loss functions detailed in the original work: a linear combination of Dice loss and Focal loss in a 1:20 ratio along with an IoU prediction loss that trains the IoU prediction head of SAM. This loss is computed as the mean square error (MSE) between the IoU prediction of SAM and the IoU of the predicted mask with the ground truth mask. We use a batch size of 32 images for the semantic segmentation model and 128 images for the prompted model. Furthermore, the prompting stage uses five iterations with either point or bounding box prompting. After each iteration, one additional point is sampled from the error area and added to the prompting queue. The middle iteration (iteration 3) is done free of any prompting. This is to preserve the initial knowledge of the model.

\subsection{Predictions on the RegMaps with LineaMapper}\label{sec:assembling_preds}

We developed a dedicated stitching algorithm to allow LineaMapper to process images larger than the original tile size of 112 - 224 px.

The need for such an algorithm is common in semantic (`pixel')\sidecite{Huang2018} and instance\sidecite{Jiang2023} image segmentation applications.  %This assembly algorithm is released together with the code (section code availability). 
The workflow of this assembly algorithm is depicted in \Cref{fig:graphic_Stitching_algo}. In order to define a merging criterion for lineament predictions in 224x224 pixel tiles, we tessellate the original image into tiles that have 50~\% overlap with each neighbouring tile. 
Only if the boxes of two predictions overlap, the masks are considered for merging. If so, the algorithm tests the criteria for merging: 1) the masks have to reach a specified ratio of mask intersection over mask union (IoU) and 2) the difference of their azimuths must be within a certain range. Furthermore, if two lineaments have the same label and one lineament overprints another (up to 10~\% overlap accuracy), the lineaments are merged. Subpolygons with less than 15 polygon points are considered artefacts/noise and are deleted. The algorithm assesses iteratively which combination of masks to merge and assigns the most abundant category label to the merged lineament. For the final production of the regional maps, we used the IoU multiplied by a factor of 9 and an azimuth range of 5$^{\circ}$ as the criterion to merge lineaments.
Challenges of the stitching algorithm include: 1) One lineament may have two masks that overlap, but did not fulfill the IoU criterion for merging. 2) If the azimuth of a lineament changes more inside a 224x224~px area (48~km) than the specified azimuth limit for merging, lineament segments might not be stitched together. 3) Lineaments with interruptions longer than half the tile size, e.g. due to cross-cuttings by other lineaments, might not be merged because their IoU may be too low. Therefore, manual revision is needed. 

\subsection{Extraction and analysis of lineament characteristics}\label{Sect_extraction_algo}
% Extraction algorithm:
% mention companion paper
We use an algorithm to extract the length, width, mean longitude, and latitude of all lineaments, their azimuth, and the number of fragmentations per kilometer length (FPK). The FPK can be used as a proxy for age when we assume that more fragmentations correlate to older lineaments. However, since an FPK of 0.1 results from, for example, a 100~km long lineament with 10 fragmentations, and also from a 10~km long lineament with 1 fragmentation, we also need to consider the lineament length when making statements about the FPK. Else, we make the implicit assumption that the relative age of long and heavily fragmented lineaments is equal to short and mildly fragmented lineaments.

One current pitfall of our extraction algorithm is that, due to the tessellation into 2$^{\circ}$x2$^{\circ}$ orthographically projected tiles, it may happen that one lineament is cut parallel to its azimuth. This happens predominantly for wide lineaments and leads to a slight overestimation of their length and an underestimation of their width. The length is measured across disruptions and fragmentation boundaries.

A Bayesian analysis is conducted to find best fitting posterior distributions of and correlations between lineament characteristics, given the data. Weak, uninformed prior distributions (normal distributions for slopes, half normal for standard deviation $\sigma$) are chosen, which do not assume either a positive or negative correlation a priori. We choose a Gamma-distribution parameterised with the mean $\mu$ and variance $\sigma$ for the likelihood of width, length, and FPK, because these variables are all naturally non-negative and right-skewed (median is smaller than the mean). 

The mean is estimated by fitting a Gamma distribution to the observed values:
\begin{align}\label{eq_gamma_GLM}
    y_i  &\sim \text{Gamma}(\mu, \sigma),\\
    \log(\mu) &= \beta_0,\\
    \beta_0  &\sim \mathcal{N}(\mu_0,\sigma_0). \\
\end{align}
For the number of fragmentations, we use a Poisson distribution to account for the discrete values, with $\lambda_i$ as the expectation value:
\begin{align}\label{eq_poisson_GLM}
    y_i  &\sim \text{Poisson}(\lambda_i),\\
    \log(\lambda_i) &= \beta_0, \\ % + \beta_1 x_i,\\
    \beta_0  &\sim \mathcal{N}(\mu_0,\sigma_0).
    %\beta_1 &\sim \mathcal{N}(\mu_1,\sigma_1).
\end{align}
To test for correlations, we construct a Bayesian generalised linear model for the outcome variable $y$ with a logarithm as the link function:
\begin{align}
    y_i  &\sim \text{Gamma}(\mu, \sigma),\\
    \log(\mu) &= \beta_0 + \beta_1 x_i,\\
    \beta_0  &\sim \mathcal{N}(\mu_0,\sigma_0), \\
    \beta_1 &\sim \mathcal{N}(\mu_1,\sigma_1),
\end{align}
with the predictor variable $x$ (observed data points are denoted as $x_i$). For observed values $y_i~=~0$, we add an increment of $10^{-4}$ for numerical stability. 

\begin{table*}[]
\centering
\caption{Bounding box and mask precision and recall for the test dataset shown in \Cref{fig:full_map}, tiled to 112x112 pixel tiles. Data is shown in percent and for detections with a score higher or equal to 0.5 and at an IoU threshold of 0.5.}
\label{tab:prec_rec_iou05}
\begin{tabular}{@{}lP{1.8cm}P{1.8cm}P{1.8cm}P{1.8cm}P{1.8cm}P{1.8cm}@{}}
\toprule
 & \textbf{Box Precision} & \textbf{Box Recall} & \textbf{v2.0 Mask Precision} & \textbf{v2.0 Mask Recall} & \textbf{GR Mask Precision} & \textbf{GR Mask Recall} \\ \midrule
Bands & 33.0 & 22.0 & 28.0 & 18.0 & 27.0 & 22.0 \\
Double ridges & 36.0 & 39.0 & 30.0 & 30.0 & 52.0 & 35.0 \\
Ridge complexes & 31.0 & 37.0 & 26.0 & 33.0 & 41.0 & 34.0 \\
Undiff. lineae & 26.0 & 27.0 & 19.0 & 19.0 & 28.0 & 17.0 \\ \midrule
Average & 31.5 & 31.2 & 25.8 & 25.0 & 37.0 & 27.0 \\ \bottomrule
\end{tabular}
\end{table*}

%\clearpage

