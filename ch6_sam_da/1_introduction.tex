\section{Introduction}
\label{sec:introduction}

Domain adaptation for semantic segmentation in medical imaging is vital to ensure that models perform effectively across different domains (\eg, different medical centers or different scanning protocols). This is critical for the practical deployment of these models in real-world medical settings where training data is often collected over a limited number of sites or settings but needs to generalize broadly. In the context of medical imaging, two domain adaptation settings are particularly interesting: (1) {\it fully supervised}, where a general model is fine-tuned with source domain images and annotations in the hope it generalizes to a target domain, and (2) {\it test-time adaptation} whereby a single target domain image is available to fine-tune a general model. While the former case needs to generalize the model to other domains via fine-tuning, the latter adapts the general model to a specific test time case. In this paper, we consider both cases. 

At the same time, recent foundational segmentation models, such as Segment Anything Model (SAM)~\sidecite{sam} or Segment-Everything-Everywhere Model (SEEM)~\sidecite{zou2024segment}, accept prompts of different input types to yield impressive zero-shot segmentation performance. Despite their capabilities on natural images, they fail to demonstrate similar prowess with medical domain images~\sidecite{zhang2023comprehensive}. 
However, recent works have attempted to address this by fine-tuning models end-to-end, for instance,~\sidecite{MedSAM,zhang2023customized} fully fine-tuned the model for 3D medical data, which led to high computational and memory costs, while only adding marginal benefits when training from natural image based pre-trained models~\sidecite{raghu2019transfusion,zamir2018taskonomy}. Regardless, such strategies only offer temporary workarounds as increased model complexity and training data sizes suggest that end-to-end training may only be feasible for entities with ample resources.

An enticing alternative are Parameter Efficient Fine-Tuning methods that minimize the number of trainable parameters while achieving comparable performances to full fine-tuning~\sidecite{hu2022lora,lin2020exploring,mahabadi2021parameter,pfeiffer2020adapterfusion}. Originally developed within the domain of Natural Language Processing (NLP), these techniques are extending to image segmentation~\sidecite{chen2023sam,chen2022vision}, fueled by impractical training times associated with Large Language Models (LLM)~\sidecite{he2021towards,li2021prefix}. For instance, Medical SAM Adapter~\sidecite{wu2023medical} proposes three methods in increasing complexity: (i) incorporate a traditional adapter into the image encoder, (ii) extend the adapter functionality to accommodate 3D images by duplicating certain layers, and (iii) a prompt-conditioned adaptation approach. With significantly smaller decoder models, current efforts are primarily focused on adapting the model's encoder weights. In contrast,~\sidecite{ke2024segment} proposes to improve the SAM segmentation quality via a learnable High-Quality Output Token injected into SAM's decoder that receives intermediate features from the ViT image encoder. However, little has been shown regarding the effect of performing decoder adaptation. This lies at the heart of the present work. 

We propose a novel SAM adapter that offers excellent and broad generalization capabilities due to its strategic placement in the mask decoder while simultaneously yielding improved segmentation across both fully supervised and test-time domain adaptation tasks. Our adapter is simple in nature and leverages the pre-trained model that inherently contains domain knowledge.
Consequently, neither the image encoder nor the mask decoder require significant parameter updates during the adaptation phase. By making this design choice, we significantly decrease the number of trainable parameters compared to existing methods, making it highly efficient and easy to train. We extensively validate our approach across three medical datasets and one natural image dataset. In addition, we provide comprehensive ablation studies that explore the impact of our design choices. Our results demonstrate that the SAM Decoder Adapter (SAM-DA) outperforms general methods such as LoRA~\sidecite{hu2022lora} and HQ-SAM~\sidecite{ke2024segment}, and also medical-specific methods (\eg~ Med-SA~\sidecite{wu2023medical}) on both fully supervised segmentation and test-time domain adaptation. Particularly noteworthy is that this superior performance is achieved by training less than $1\%$ of the total SAM parameters.
