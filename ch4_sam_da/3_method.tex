\section{Method}
\label{sec:method}
%% Main points:
%% - Preliminary: explain SAM very briefly and how the decoder works
%% - Explain the adapter
%% - How we attach the adapter to the decoder of SAM
%% - Initialization of the adapter
%% - Why we use it in the decoder: less parameters. The encoder should give a good representation already
%% - Image showing everything
We propose a simple, lightweight adapter for the Segment Anything Model (SAM)~\sidecite{sam} that draws inspiration from adaptation techniques in the NLP literature. 

\plainwidefig[t]{1}{Figures/method.pdf}{Illustration of the proposed adaption for SAM Decoder in layer~$\ell$. In each layer, the adaption embeddings are fed along with the dense embeddings to the trainable zero-initialized attention module. Then, the resulting tokens are combined with the decoder embeddings with a trainable gating parameter and a linear MLP.}{fig:method_samda}

\subsection{Segment Anything Model.}
\label{sec:sam}
SAM consists of three primary components: an image encoder, a prompt encoder, and a mask decoder. The image encoder is a standard MAE pre-trained Vision Transformer (ViT)~\sidecite{dosovitskiy2021an} that transforms the input image into an embedding space. The prompt encoder takes either sparse (points, boxes) or dense (masks) annotation formats and produces encoded prompts. Both the image embedding and the encoded prompts are fed to the mask decoder, which consists of a transformer block with a mask prediction head. The transformer block applies two layers of two-way cross-attention operations acting on the image and the prompt embeddings. The result of the transformer is upsampled with an MLP and then fed to a linear classifier, which predicts the final segmentation mask.

\subsection{SAM Decoder Adapter.}
\label{sec:adapter}

The LLaMA-Adapter~\sidecite{llama_adapter} is an adaption method originally introduced to finetune pre-trained LLMs such as LLaMA~\sidecite{llama}. In the context of language generation, this adapter introduces a set of learnable adaption prompts at the higher layers of the LLaMA transformer, and prepends them to the word tokens before the attention operations. 

Our approach brings the idea of prompt-based adaptation from NLP to SAM. We introduce a new learnable adaption prompt~$A_\ell\in\real^{N\times{}D}$ at each layer~$\ell$ of the mask decoder's transformer. The adaptation prompts are used to compute correction factors that modify the embeddings of the transformer without retraining its parameters. Formally, let~$T_\ell\in\real^{M\times D}$ be the embeddings obtained as the output of the cross-attention operation at layer~$\ell$.

We feed $A_\ell$ and~$T_\ell$ to an additional attention block, where the embeddings $T_\ell$ act as queries and the adapter weights~$A_\ell$ act as keys and values\sidenote{We choose to employ the image tokens as queries and the adapter as keys because the image tokens contain important contextual information that should influence the adaptation process. We reinforce this behavior by doing so.},
\begin{eqnarray}
    Q_\ell & = & \linear^q_\ell(T_{\ell}) \in \real^{M\times D_k}, \\
    K_\ell & = & \linear^k_\ell(A_{\ell}) \in \real^{N\times D_k}, \\
    V_\ell & = & \linear^v_\ell(A_{\ell}) \in \real^{N\times D_v}.
\end{eqnarray}
The attention scores are calculated as usual,
\begin{equation}
    S_\ell = \softmax\left(\dfrac{Q_\ell{}K_\ell^T}{\sqrt{D_v}}\right)V_\ell \in \real^{M\times D_v},
\end{equation}
and projected back to the model dimension~$D$ with a linear layer, $S'_\ell=\linear^o_\ell(S_\ell)$. The result~$S'_\ell$ serves as the correction factor of the original embeddings,
\begin{equation}
    T'_\ell = \linear^t_\ell(T_\ell + g_\ell \cdot S'_\ell),
\end{equation}
where the learnable gating factor~$g_\ell\in\real$ is initialized to~0 to ensure no disruption during the early stages of adaptation learning. The entire procedure is summarized in \cref{fig:method_samda,fig:neural_diagram}\sidenote{\cref{fig:neural_diagram} has been created following~\cite{abbott2024neural}, a work that proposes a new standard for neural diagram representation.}.

\input{Figures/neural_diagram}

We note that this approach could potentially be implemented within the model's encoder. However, we deliberately decided not to pursue this route for two main reasons: (1) the image representation generated by the encoder is already high-quality due to a pre-trained model on similar data, and (2) any modifications to the encoder's parameters may necessitate retraining of the mask decoder as well. Given our objective of reducing the number of parameters, we confine the implementation solely to the decoder. In \Cref{sec:ablation_samda}, we show experimentally how the location of the adapter affects the model's performance.
