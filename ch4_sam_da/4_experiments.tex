\section{Experimental setup}
\label{sec:experiments}
%% Points:
%% - Datasets: Retouch, MRI (Binary), ...
%% - Hyperparameters are tuned for each experiment on the validation set, then applied to testing. Probably a table with the hyperparams doesn't fit
%% - Baselines
%% - Supervised training, TTDA
The following section details our experimental setup and compares our approach to several baselines. We apply our method to two scenarios: fully-supervised semantic segmentation and test-time domain adaptation (TTDA) for semantic segmentation on four datasets.

\subsection{Datasets}
We validate our approach on one natural image dataset and three medical cross-device/site datasets:
\begin{itemize}
    \item{\textbf{Retouch}\sideauthorcite{retouch}}: Retinal OCT volumes from three devices - Cirrus, Spectralis, and Topcon devices - including segmentation masks for three biological markers: IRF, SRF, and PED. We use ``Spectralis'' dataset as the source and ``Cirrus'' dataset as the target domain. 
    
    % Retinal OCT volumes acquired with three different devices: Cirrus HD-OCT (Zeiss, Meditec), Spectralis (Heidelberg Engineering), and T-1000/T2000 (Topcon). The volumes contain segmentation masks for three retinal biological markers: IRF, SRF, and PED. For TTDA, we use ``Spectralis'' dataset as the source and ``Cirrus'' dataset as the target domain.
    
    \item{\textbf{MRI}\sideauthorcite{liu2020saml,mri_dataset}}: Prostate MRI scans from various sites with different devices showing prostate capsule segmentation at various cancer stages. We extract subsets from Boston Medical Center (BMC) and University College London (UCL). ``BMC'' serves as the source domain, while ``UCL'' is the target.
    
    %Prostate T2-weighted MRI acquired at multiple sites with different devices. Each MRI contains a segmentation of the prostate capsule at different stages of cancer. We extract two subsets from different sites: Boston Medical Center (BMC) and University College London (UCL). For TTDA, we sample volumes from ``BMC'' and ``UCL'' as the source and target domain, respectively. 
    
    \item{\textbf{WMH}\sideauthorcite{wmh}}: Brain MRI scans from three sites: NUHS Singapore, UMC Utrecht, and VU Amsterdam, with White Matter Hyperintensities and other pathologies segmented. We extract FLAIR axial slices containing segmentation. We take ``UMC Utrecht'' as the source, and ``NUHS Singapore'' as the target domain.
    
    % Brain T1-weighted and FLAIR MRI acquired at three sites with different devices: National University Health System in Singapore (NUHS Singapore), University Medical Center Utrecht (UMC Utrecht), and VU University Medical Centre Amsterdam (VU Amsterdam). Each MRI contains a segmentation of WMH and other pathologies (grouped). We extract all axial slices from the FLAIR MRI containing segmentation. For TTDA, we sample volumes from ``UMC Utrecht'' and ``NUHS Singapore'' as the source and target domain, respectively. 

    \item{\textbf{HQSeg-44K}\sideauthorcite{ke2024segment}} contains a collection of datasets for training and testing. For training, data is taken from DIS\sidecite{qin2022highly} (train set), ThinObject-5K\sidecite{coift_liew2021deep} (train set), FSS-1000\sidecite{fss_li2020fss}, ECSSD\sidecite{eccsd_shi2015hierarchical}, MSRA10K\sidecite{msra_cheng2014global} and DUT-OMRON\sidecite{dutomron_yang2013saliency}. The models are tested on a collection of four datasets: DIS (validation set), ThinObject-5K (test set), COIFT\sidecite{coift_liew2021deep}, and HRSOD\sidecite{hrsod_zeng2019towards}. These datasets contain fine-grained mask labels of natural images and together they add up to more than 1,000 semantic classes. 
    
\end{itemize}
In all cases, we use the source domain as the only domain for the fully supervised experiments. Since HQSeg-44K contains natural images with no clear domain shift, we do not report results for generalization and domain adaptation on this dataset. We report all the results on separate test sets, following hyperparameter fine-tuning on a validation set and repeating each experiment four times. \Cref{tab:dataset_details_samda} describes the size of the training, validation, and test splits for each dataset and domain. 

\begin{table*}[]
\centering
\caption{Number of images for each dataset per split. HQSeg-44K is a combination of datasets~\cite{ke2024segment} and it uses the same set for validation and testing.}
\label{tab:dataset_details_samda}
\begin{tabular}{@{}p{2.5cm}lP{2.2cm}P{2.2cm}P{2.2cm}@{}}
\toprule
\multicolumn{1}{l}{\textbf{Dataset}} & \textbf{Domains} & \textbf{Training} & \textbf{Validation} & \textbf{Test} \\ \midrule
\multirow{2}{*}{Retouch~\cite{retouch}} & Spectralis & 1,773 & 591 & 480 \\
 & Cirrus & 3,765 & 1,255 & 1,252 \\ \midrule
\multirow{2}{*}{MRI~\cite{liu2020saml,mri_dataset}} & BMC & 177 & 92 & 93 \\
 & UCL & 82 & 41 & 41 \\ \midrule
\multirow{2}{*}{WMH~\cite{wmh}} & Utrecht & 4,260 & 1,065 & 1,255 \\
 & Singapore & 3,560 & 890 & 1,105 \\ \midrule
HQSeg-44k~\cite{ke2024segment} & - & 44,320 & \multicolumn{2}{c}{1,537} \\
 
 \bottomrule
\end{tabular}

\end{table*}

\subsection{Baselines}
We compare our adapter against four alternative techniques. Among these, three are prominent examples of contemporary state-of-the-art approaches: LoRA\sidecite{hu2022lora}, Med-SA\sidecite{wu2023medical} and HQ-SAM\sidecite{ke2024segment}. The remaining methods involve completely fine-tuning the model and solely fine-tuning the decoder while keeping the encoder frozen. It is important to acknowledge that certain methods alter the image embedding as they are integrated within the encoder. To ensure equitable comparisons, we train both the adapter and the mask decoder in these cases. We use the ViT-B/16\sidecite{dosovitskiy2021an} variant of the Vision Transformer pre-trained with MedSAM\sidecite{MedSAM} weights for all our experiments on medical datasets and the official SAM pre-training weights for HQSeg-44K.


\subsection{Fully Supervised Training Experiments}
Here, we evaluate SAM Decoder Adapter through fully supervised semantic segmentation training on individual images and evaluate the models on the same training domain and a different domain\sidenote{Unlike previous literature on SAM, we perform all of our experiments without prompting, and leave prompting adaption to further work.}. 
We use AdamW\sidecite{loshchilov2017decoupled} optimizer in all cases with a loss that combines a \emph{mask prediction loss} and a \emph{IoU prediction loss}. The mask prediction loss is a linear combination of Dice loss\sidecite{milletari2016dice} and Cross Entropy loss in a 0.8:0.2 ratio. The IoU prediction loss is used to train the IoU prediction head of SAM. It is computed as the MSE between the IoU prediction of SAM and the IoU of the predicted mask with the ground truth mask. As reported in~\yeartextcite{sam}, using this IoU prediction loss with a weight of 1.0 increases performance slightly. Other hyperparameters are fine-tuned with the validation set. Our adapter uses $N=2$~tokens and dimension~$D=512$. The dimensions of the attention module are~$D_v=D_k=256$.

\subsection{Test-Time Domain Adaptation Experiments}
We adopt a conventional training approach centered on entropy minimization per sample, leveraging the most confident samples\sidecite{wang2021tent}. Then, we enforce proximity between the output and initial predictions through a regularization term incorporating focal loss\sidecite{lin2017focal} and dice loss, following \yeartextcite{zhang2023improving}. Finally, for Retouch we introduce a contrastive term within the volume. Negative slices are sampled distantly from the anchor, while positive slices are nearby. We optimize the contribution of each term to the final loss for every method and dataset on the validation set, as well as the number of iterations. We zero-initialize the adapter of the Med-SA baseline, ensuring that it does not affect the model predictions before adaptation. Due to the challenge of zero-initializing HQ-SAM, we omit it from these experiments.
