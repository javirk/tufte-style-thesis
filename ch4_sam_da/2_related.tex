\section{Related work}
\label{sec:samda_related}
% Related work in the following:
% - SAM for medical domain
% - Fine-tuning/prompt tuning
% - TTDA
\subsection{Parameter-Efficient Fine-Tuning} 
Parameter Efficient Fine-Tuning (PEFT) methods have emerged as a response to the increasing model size. These techniques aim to minimize the number of trainable parameters while achieving comparable performance to full fine-tuning. By training only a subset of parameters, PEFT methods aim to retain the knowledge of the base model when trained on a secondary task, thereby mitigating issues like catastrophic forgetting and overfitting, especially when dealing with smaller target datasets. The landscape of PEFT methodologies is vast\sidecite{xu2023parameter}, with LoRA\sidecite{hu2022lora} standing out as a resilient method over time. LoRA incorporates two trainable low-rank matrices for weight updates. Specifically, it employs a down-projection matrix and an up-projection matrix alongside the query, key, and value matrices within the attention layer of the transformer.

Others have envisioned adaptation methods that function as plugins for large models. These emerged along with large-scale models in the Natural Language Processing literature\sidecite{houlsby2019parameter} and have subsequently spread to other fields of Deep Learning. The fundamental concept underlying these methods is to insert a module with few parameters into the base model and solely update those while maintaining the pre-trained model frozen. Initially pioneered by the Adapter framework\sidecite{houlsby2019parameter}, this approach inserts such modules sequentially after the self-attention layer in all transformer layers. Since then, various other methodologies have emerged, with adaptations in the position of the adapter\sidecite{pfeiffer2020adapterfusion,mahabadi2021parameter,lin2020exploring}. This evolution has also transpired in semantic segmentation, where large models are beginning to establish their significance\sidecite{chen2022vision,chen2023sam}. The medical domain presents formidable challenges and has witnessed recent advancements, exemplified by works like Medical SAM Adapter\sidecite{wu2023medical}. Here, Wu et al. develop three methods in increasing complexity: firstly, they incorporate a traditional adapter into the image encoder; secondly, they extend adapter functionality to accommodate 3D images by duplicating certain layers; finally, they propose a prompt-conditioned adaptation approach. More recently, \cite{ke2024segment} proposes a method to improve the quality of SAM segmentation masks via a learnable High-Quality Output Token injected into SAM's decoder that receives features from the ViT image encoder.

Lastly, certain methods leverage the tokenization process of LLMs and implement PEFT with prompt tuning\sidecite{lester2021power}. One such method, LLaMA-Adapter\sidecite{llama_adapter}, was explicitly developed to fine-tune LLaMA\sidecite{llama} into an instruction-following model. Specifically, in the higher transformer layers of LLaMA, they append a set of trainable, zero-initialized adaptation prompts as a prefix to the input instruction tokens. 

\subsection{Domain Adaptation for Semantic Segmentation}
Domain adaptation has a rich history owing to its practical utility in reducing annotation requirements in unseen domains. Techniques like Unsupervised Domain Adaptation aim to do so without target labels\sidecite{ghamsarian2023domain,hoyer2023mic,chen2023pipa}, Source Free Domain Adaptation restricts the task even more by removing access to the source domain annotated data\sidecite{zhang2023improving,kundu2020universal,xia2021adaptive}, and Test-Time Domain Adaptation (TTDA) focuses on a setting in which data is received online and the model has to adapt per sample\sidecite{janouskova2023single,wang2023dynamically}. Large models aim to mitigate the need for domain adaptation by training on extensive datasets, allowing them to generalize to unseen domains from others that are present in the dataset\sidecite{oquab2023dinov2}. To our knowledge, the effectiveness of these approaches when applied to SAM has not been extensively investigated, with only a few studies such as \cite{zhang2023improving} exploring this area. In this work, the authors propose a weakly supervised self-training architecture to enhance the robustness and computation efficiency of the adaptation.
