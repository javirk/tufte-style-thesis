\section{Results}
\label{sec:samda_results}
%% Points:
%% - Supervised experiments (table 1) + zero-shot (table 2)
%% - TTDA experiments (table 3)
%% - Some images showing the evolution <-- Save ckpts for some imgs then

\plainwidefig[t]{1}{Figures/full_supervision_MICCAI.pdf}{Qualitative results on eight randomly selected test samples.}{fig:full_superv}

\subsection{Full Supervision}
\Cref{tab:supervised_training} presents a comparison of the IoU scores achieved by SAM Decoder Adapter (SAM-DA) against alternative baselines across four datasets in the fully supervised task. Our approach consistently delivers comparable or superior results to full fine-tuning despite employing only a fraction of the trainable parameters\sidenote{See \Cref{tab:trainable_params}}. Notably, the number of training images influences the model's final performance: the BMC domain in the MRI dataset, with the fewest samples across all datasets, showcases significant performance improvement with our adapter compared to other methods. With fewer parameters, SAM Decoder Adapter is less susceptible to overfit to the training set, thereby retaining valuable knowledge from the pre-trained weights. This effect diminishes as the dataset size increases, where the advantage of the adapter over competitive baselines is less evident. \cref{fig:full_superv} shows qualitative results of our adapter.

\input{tables/full_supervision}

\input{tables/domain_generalization}



Due to the substantial number of images in HQSeg-44K\sideauthorcite{ke2024segment}, this dataset can be considered quite distinct. With 44k training images, encoder-adaptation methods are expected to outperform decoder-only approaches due to their higher number of parameters. Our method achieves an IoU score of 79.6 on this dataset, falling behind LoRA and Med-SA (with an average IoU of 83.5). However, it surpasses other decoder-only adapters like HQ-SAM, which achieves an IoU of 79.2. These results are the average over the four testing sets in HQSeg-44K, and the detailed figures can be found on \Cref{tab:hqseg_results_samda}.

\subsection{Domain Generalization}
One of the primary strengths of SAM lies in its ability to zero-shot transfer thanks to its extensive pre-training corpus. We investigate in \Cref{tab:zero_shot} whether this capability is retained after the adaption by testing the methods from \Cref{tab:supervised_training} on previously unseen domains within each medical dataset. SAM Decoder Adapter demonstrates statistically significant superiority in zero-shot generalization compared to other methods that primarily focus on traditional encoder adaptation, such as Med-SA and LoRA, and shows on-par performance to fine-tuning in the low-data regime. See supplementary material for qualitative results of our adapter compared to two baselines on the generalization domains.

\subsection{Ablation study}\label{sec:ablation_samda}
The Vision Transformer-type architecture used in the backbone of SAM facilitates the seamless integration of the proposed adapter into the encoder with minimal adjustments compared to the configuration depicted in \cref{fig:method_samda}. We adopt the approach outlined in \yeartextcite{llama_adapter} to position the adapter within the encoder, focusing on adapting only the last layers. Additionally, we utilize all image embeddings as queries for the attention module and fine-tune the decoder, following the setting from LoRA and Med-SA. ViT-B comprises 12 blocks, and we modify the last ten blocks in our adaptation approach. This adaptation strategy allocates $17.7$M trainable parameters to the encoder adaptation, in addition to $3.9$M parameters in the decoder. \Cref{tab:encoder_ablation_trained,tab:encoder_ablation_generalization} illustrate the impact of adapting the encoder layers compared to the proposed method. The results confirm that locating the adapter in the decoder improves generalization on unseen domains\sidenote{As seen in \Cref{tab:encoder_ablation_generalization}.}, especially in the case of Retouch and WMH datasets, where we see a gain of $8.5\%$ and $13.8\%$ in IoU score, respectively.

\input{tables/encoder_ablation_trained}

\input{tables/encoder_ablation_generalization}

\Cref{tab:ablation_size,tab:ablation_size_transfer} in the appendix show the impact of the dimension of the adaptation prompt $A_\ell$ on the performance. The results do not suggest that the size of the adaptation prompt impacts the full supervision or the zero-shot generalization performance for the medical datasets significantly. For HQSeg-44K, however, the drop in performance when the size is increased is blatant. This is in line with the previous observation that decoder-only methods cannot trace encoder-decoder approaches in large datasets, and further adding parameters only promotes overfitting to the training set.

\subsection{Test-Time Domain Adaptation}
Due to the low number of trainable parameters, test-time domain adaptation is the ideal setting to test how much an adapter can affect the performance of a single test sample. We show in \Cref{tab:ttda} that our adapter performs better than other baselines in most cases. For MRI, the increase in trainable parameters already in LoRA hurts the initial model and decreases its IoU score after only five iterations. Our method, however, shows strong results in all three cases, as shown in \Cref{fig:ttda_samda} in the appendix.

\input{tables/ttda}