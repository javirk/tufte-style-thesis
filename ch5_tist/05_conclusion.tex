\section{Conclusion}
\label{sec: conclusion}

We proposed a novel self-training framework with a self-assessment strategy for pseudo-label reliability, namely ``Transformation-Invariant Self-Training'' (TI-ST). This method uses transformation-invariant highly-confident predictions in the target dataset by considering an ensemble of high-confidence predictions from transformed versions of identical inputs. We experimentally show the effectiveness of our approach against numerous existing methods across three different source-to-target segmentation tasks, and when using different model architectures. Beyond this, we show that our approach is resilient to changes in the methods hyperparameter, making it well-suited for different applications. 