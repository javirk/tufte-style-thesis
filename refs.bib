%% Revised: 12 September 2023
%% Copyright John H. Lienhard, MIT
%% Offered under the MIT license: https://ctan.org/license/mit 

%% REFER TO biblatex documentation for details about possible fields
%% bibtex support depends on the bibtex style (.bst) and is usually more limited
@inproceedings{Liuc,
	title        = {Partial Convolution based Padding},
	author       = {Guilin Liu and Kevin J. Shih and Ting-Chun Wang and Fitsum A. Reda and Karan Sapra and Zhiding Yu and Andrew Tao and Bryan Catanzaro},
	year         = 2018,
	booktitle    = {arXiv preprint arXiv:1811.11718}
}
@article{Kurmann2019a,
	title        = {{Expert-level Automated Biomarker Identification in Optical Coherence Tomography Scans}},
	author       = {Kurmann, Thomas and Yu, Siqing and M{\'{a}}rquez-Neila, Pablo and Ebneter, Andreas and Zinkernagel, Martin and Munk, Marion R. and Wolf, Sebastian and Sznitman, Raphael},
	year         = 2019,
	month        = {12},
	journal      = {Scientific Reports},
	publisher    = {Nature Publishing Group},
	volume       = 9,
	number       = 1,
	pages        = {1--9},
	doi          = {10.1038/s41598-019-49740-7},
	issn         = 20452322,
	pmid         = 31537854
}
@inproceedings{Kurmann2019,
	title        = {Fused detection of retinal biomarkers in OCT volumes},
	author       = {Kurmann, Thomas and M{\'a}rquez-Neila, Pablo and Yu, Siqing and Munk, Marion and Wolf, Sebastian and Sznitman, Raphael},
	year         = 2019,
	booktitle    = {Medical Image Computing and Computer Assisted Intervention--MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13--17, 2019, Proceedings, Part I 22},
	pages        = {255--263},
	organization = {Springer}
}
@inproceedings{Fan2020,
	title        = {{Positive-Aware Lesion Detection Network with Cross-scale Feature Pyramid for OCT Images}},
	author       = {Fan, Dongyi and Zhang, Chengfen and Lv, Bin and Wang, Lilong and Wang, Guanzheng and Wang, Min and Lv, Chuanfeng and Xie, Guotong},
	year         = 2020,
	booktitle    = {MICCAI}
}
@inproceedings{Tan2019,
	title        = {Efficientnet: Rethinking model scaling for convolutional neural networks},
	author       = {Tan, Mingxing and Le, Quoc},
	year         = 2019,
	booktitle    = {International conference on machine learning},
	pages        = {6105--6114},
	organization = {PMLR}
}
@inproceedings{Selvaraju2019,
	title        = {Grad-cam: Visual explanations from deep networks via gradient-based localization},
	author       = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {618--626}
}
@inproceedings{Cheung2010,
	title        = {{Diabetic retinopathy}},
	author       = {Cheung, Ning and Mitchell, Paul and Wong, Tien Yin},
	year         = 2010,
	booktitle    = {The Lancet},
	publisher    = {Elsevier Limited},
	volume       = 376,
	number       = 9735,
	pages        = {124--136},
	doi          = {10.1016/S0140-6736(09)62124-3},
	issn         = {01406736},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Cheung, Mitchell, Wong - 2010 - Diabetic retinopathy.pdf:pdf},
	pmid         = 20580421
}
@article{Yim2020,
	title        = {{Predicting conversion to wet age-related macular degeneration using deep learning}},
	author       = {Yim, Jason and Chopra, Reena and Spitz, Terry and Winkens, Jim and Obika, Annette and Kelly, Christopher and Askham, Harry and Lukic, Marko and Huemer, Josef and Fasler, Katrin and Moraes, Gabriella and Meyer, Clemens and Wilson, Marc and Dixon, Jonathan and Hughes, Cian and Rees, Geraint and Khaw, Peng T. and Karthikesalingam, Alan and King, Dominic and Hassabis, Demis and Suleyman, Mustafa and Back, Trevor and Ledsam, Joseph R. and Keane, Pearse A. and {De Fauw}, Jeffrey},
	year         = 2020,
	month        = {6},
	journal      = {Nature Medicine},
	publisher    = {Nature Research},
	volume       = 26,
	number       = 6,
	pages        = {892--899},
	doi          = {10.1038/s41591-020-0867-7},
	issn         = {1546170X},
	pmid         = 32424211
}
@article{Hu2019,
	title        = {Squeeze-and-Excitation Networks},
	author       = {J. {Hu} and L. {Shen} and S. {Albanie} and G. {Sun} and E. {Wu}},
	year         = 2020,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 42,
	number       = 8,
	pages        = {2011--2023},
	doi          = {10.1109/TPAMI.2019.2913372}
}
@article{Domalpally2018,
	title        = {{Evaluation of Diabetic Retinopathy Using the ETDRS Severity Scale - Is There A Gold Standard?}},
	author       = {Domalpally, Amitha and Trane, Ralph and Reimers, James and Blodi, Barbara A.},
	year         = 2018,
	journal      = {Investigative Ophthalmology and Visual Science},
	volume       = 59,
	number       = 9,
	pages        = {4676--4676}
}
@article{Kullback1951a,
	title        = {{On Information and Sufficiency}},
	author       = {Kullback, S. and Leibler, R. A.},
	year         = 1951,
	journal      = {The Annals of Mathematical Statistics},
	doi          = {10.1214/aoms/1177729694},
	issn         = {0003-4851},
	abstract     = {Volume 22, Number 1 (1951), 1-164}
}
@article{Russakovsky2015,
	title        = {{ImageNet Large Scale Visual Recognition Challenge}},
	author       = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	year         = 2015,
	journal      = {International Journal of Computer Vision},
	doi          = {10.1007/s11263-015-0816-y},
	issn         = 15731405,
	abstract     = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5Â years of the challenge, and propose future directions and improvements.},
	archiveprefix = {arXiv},
	arxivid      = {1409.0575},
	eprint       = {1409.0575},
	keywords     = {Benchmark,Dataset,Large-scale,Object detection,Object recognition}
}
@inproceedings{Kurmann,
	title        = {{Deep Multi-Label Classification in Affine Subspaces}},
	author       = {Kurmann, Thomas and {M{\'{a}}rquez Neila}, Pablo and Wolf, Sebastian and Sznitman, Raphael},
	year         = 2019,
	booktitle    = {MICCAI}
}
@book{Trucco2020,
	title        = {{Computational Retinal Image Analysis}},
	author       = {Trucco, Emanuele and MacGillivray, Tom and Xu, Yanwu},
	year         = 2020,
	publisher    = {Elsevier}
}
@article{Lee2018,
	title        = {{Automated Segmentation of Lesions Including Subretinal Hyperreflective Material in Neovascular Age-related Macular Degeneration}},
	author       = {Lee, Hyungwoo and Kang, Kyung Eun and Chung, Hyewon and Kim, Hyung Chan},
	year         = 2018,
	journal      = {American Journal of Ophthalmology},
	volume       = 191,
	pages        = {64--75},
	doi          = {10.1016/j.ajo.2018.04.007}
}
@article{DeZanet2020,
	title        = {{Automated detection and quantification of pathological fluid in neovascular age-related macular degeneration using a deep learning approach}},
	author       = {{De Zanet}, S and Mosinska, A and Bergin, C and {Sole Polito}, M and Guidotti, J and Apostolopoulos, S and Ciller, C and Mantel, I},
	year         = 2020,
	journal      = {Investigative Ophthalmology and Visual Science},
	volume       = 61,
	number       = 7,
	issn         = {1552-5783}
}
@article{Liefers2021,
	title        = {{Quantification of key retinal features in early and late age-related macular degeneration using deep learning}},
	author       = {Liefers, Bart and Taylor, Paul and Alsaedi, Abdulrahman and Bailey, Clare and Balaskas, Konstantinos and Dhingra, Narendra and Egan, Catherine A. and Rodrigues, Filipa Gomes and Gonzalo, Cristina Gonz{\'{a}}lez and Heeren, Tjebo F.C. and Lotery, Andrew and M{\"{u}}ller, Philipp L. and Olvera-Barrios, Abraham and Paul, Bobby and Schwartz, Roy and Thomas, Darren S. and Warwick, Alasdair N. and Tufail, Adnan and S{\'{a}}nchez, Clara I.},
	year         = 2021,
	journal      = {American Journal of Ophthalmology},
	doi          = {10.1016/j.ajo.2020.12.034}
}
@article{zur2017,
	title        = {{OCT Biomarkers as Functional Outcome Predictors in Diabetic Macular Edema Treated with Dexamethasone Implant}},
	author       = {Zur, Dinah and Iglicki, Matias and Busch, Catharina and Invernizzi, Alessandro and Mariussi, Miriana and Loewenstein, Anat and Busch, Catharina and Cebeci, Zafer and Chhablani, Jay Kumar and Chaikitmongkol, Voraporn and Couturier, Aude and Fraser-Bell, Samantha and Giancipoli, Ermete and Iglicki, Matias and Invernizzi, Alessandro and La{\'{i}}ns, In{\^{e}}s and Loewenstein, Anat and Lupidi, Marco and Rehak, Matus and Rodriguez, Patricio and Mendes, Thais Sousa and Sala-Puigdollers, Anna and Zur, Dinah},
	year         = 2018,
	month        = {2},
	journal      = {Ophthalmology},
	publisher    = {Elsevier Inc.},
	volume       = 125,
	number       = 2,
	pages        = {267--275},
	doi          = {10.1016/j.ophtha.2017.08.031},
	issn         = 15494713,
	pmid         = 28935399
}
@article{Bourne2017,
	title        = {{Magnitude, temporal trends, and projections of the global prevalence of blindness and distance and near vision impairment: a systematic review and meta-analysis}},
	author       = {Bourne, Rupert R.A. and Flaxman, Seth R. and Braithwaite, Tasanee and Cicinelli, Maria V. and Das, Aditi and Jonas, Jost B. and Keeffe, Jill and Kempen, John and Leasher, Janet and Limburg, Hans and Naidoo, Kovin and Pesudovs, Konrad and Resnikoff, Serge and Silvester, Alex and Stevens, Gretchen A. and Tahhan, Nina and Wong, Tien and Taylor, Hugh R. and Ackland, Peter and Arditi, Aries and Barkana, Yaniv and Bozkurt, Banu and Wormald, Richard and Bron, Alain and Budenz, Donald and Cai, Feng and Casson, Robert and Chakravarthy and others},
	year         = 2017,
	month        = {9},
	journal      = {The Lancet Global Health},
	publisher    = {Elsevier Ltd},
	volume       = 5,
	number       = 9,
	pages        = {e888--e897},
	doi          = {10.1016/S2214-109X(17)30293-0},
	issn         = {2214109X},
	pmid         = 28779882
}
@article{Phadikar2017,
	title        = {{The potential of spectral domain optical coherence tomography imaging based retinal biomarkers}},
	author       = {Phadikar, Prateep and Saxena, Sandeep and Ruia, Surabhi and Lai, Timothy Y.Y. and Meyer, Carsten H. and Eliott, Dean},
	year         = 2017,
	journal      = {International Journal of Retina and Vitreous},
	publisher    = {BioMed Central Ltd.},
	volume       = 3,
	number       = 1,
	doi          = {10.1186/s40942-016-0054-7}
}
@incollection{Schlegl,
	title        = {{Predicting Semantic Descriptions from Medical Images with Convolutional Neural Networks}},
	author       = {Schlegl, Thomas and Waldstein, Sebastian M. and Vogl, Wolf-Dieter and Schmidt-Erfurth, Ursula and Langs, Georg},
	year         = 2015,
	booktitle    = {International Conference on Information Processing in Medical Imaging},
	publisher    = {Springer, Cham},
	pages        = {437--448},
	doi          = {10.1007/978-3-319-19992-4{\_}34},
	url          = {http://link.springer.com/10.1007/978-3-319-19992-4{\_}34},
	abstract     = {Learning representative computational models from medical imaging data requires large training data sets. Often, voxel-level annotation is unfeasible for sufficient amounts of data. An alternative to manual annotation, is to use the enormous amount of knowledge encoded in imaging data and corresponding reports generated during clinical routine. Weakly supervised learning approaches can link volume-level labels to image content but suffer from the typical label distributions in medical imaging data where only a small part consists of clinically relevant abnormal structures. In this paper we propose to use a semantic representation of clinical reports as a learning target that is predicted from imaging data by a convolutional neural network. We demonstrate how we can learn accurate voxel-level classifiers based on weak volume-level semantic descriptions on a set of 157 optical coherence tomography (OCT) volumes. We specifically show how semantic information increases classification accuracy for intraretinal cystoid fluid (IRC), subretinal fluid (SRF) and normal retinal tissue, and how the learning algorithm links semantic concepts to image content and geometry.}
}
@article{ma2020,
	title        = {MS-CAM: Multi-Scale Class Activation Maps for Weakly-Supervised Segmentation of Geographic Atrophy Lesions in SD-OCT Images},
	author       = {Xiao Ma and Zexuan Ji and Sijie Niu and Theodore Leng and Daniel L Rubin and Qiang Chen},
	year         = 2020,
	month        = 12,
	journal      = {IEEE Journal of Biomedical and Health Informatics},
	volume       = 24,
	pages        = {3443--3455},
	doi          = {10.1109/JBHI.2020.2999588},
	issn         = {2168-2194},
	url          = {https://ieeexplore.ieee.org/document/9121691/},
	issue        = 12
}
@article{Szeskin2021,
	title        = {A column-based deep learning method for the detection and quantification of atrophy associated with AMD in OCT scans},
	author       = {Adi Szeskin and Roei Yehuda and Or Shmueli and Jaime Levy and Leo Joskowicz},
	year         = 2021,
	month        = 8,
	journal      = {Medical Image Analysis},
	volume       = 72,
	pages        = 102130,
	doi          = {10.1016/j.media.2021.102130},
	issn         = 13618423,
	abstract     = {The objective quantification of retinal atrophy associated with age-related macular degeneration (AMD) is required for clinical diagnosis, follow-up, treatment efficacy evaluation, and clinical research. Spectral Domain Optical Coherence Tomography (OCT) has become an essential imaging technology to evaluate the macula. This paper describes a novel automatic method for the identification and quantification of atrophy associated with AMD in OCT scans and its visualization in the corresponding infrared imaging (IR) image. The method is based on the classification of light scattering patterns in vertical pixel-wide columns (A-scans) in OCT slices (B-scans) in which atrophy appears with a custom column-based convolutional neural network (CNN). The network classifies individual columns with 3D column patches formed by adjacent neighboring columns from the volumetric OCT scan. Subsequent atrophy columns form atrophy segments which are then projected onto the IR image and are used to identify and segment atrophy lesions in the IR image and to measure their areas and distances from the fovea. Experimental results on 106 clinical OCT scans (5,207 slices) in which cRORA atrophy (the end point of advanced dry AMD) was identified in 2,952 atrophy segments and 1,046 atrophy lesions yield a mean F1 score of 0.78 (std 0.06) and an AUC of 0.937, both close to the observer variability. Automated computer-based detection and quantification of atrophy associated with AMD using a column-based CNN classification in OCT scans can be performed at expert level and may be a useful clinical decision support and research tool for the diagnosis, follow-up and treatment of retinal degenerations and dystrophies.},
	keywords     = {CNN deep learning,Column-based OCT scattering,OCT scan analysis,Retinal atrophy in dry age-related macular degener},
	pmid         = 34198041
}
Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop
@article{chen2020deep,
	title        = {{Deep learning for cardiac image segmentation: a review}},
	author       = {Chen, Chen and Qin, Chen and Qiu, Huaqi and Tarroni, Giacomo and Duan, Jinming and Bai, Wenjia and Rueckert, Daniel},
	year         = 2020,
	journal      = {Frontiers in Cardiovascular Medicine},
	publisher    = {Frontiers},
	pages        = 25
}
@article{mustafa2021supervised,
	title        = {{Supervised Transfer Learning at Scale for Medical Imaging}},
	author       = {Mustafa, Basil and Loh, Aaron and Freyberg, Jan and MacWilliams, Patricia and Wilson, Megan and McKinney, Scott Mayer and Sieniek, Marcin and Winkens, Jim and Liu, Yuan and Bui, Peggy and Prabhakara, Shruthi and Telang, Umesh and Karthikesalingam, Alan and Houlsby, Neil and Natarajan, Vivek},
	year         = 2021,
	month        = {jan},
	journal      = {arXiv preprint arXiv:2101.05913},
	url          = {http://arxiv.org/abs/2101.05913},
	abstract     = {Transfer learning is a standard technique to improve performance on tasks with limited data. However, for medical imaging, the value of transfer learning is less clear. This is likely due to the large domain mismatch between the usual natural-image pre-training (e.g. ImageNet) and medical images. However, recent advances in transfer learning have shown substantial improvements from scale. We investigate whether modern methods can change the fortune of transfer learning for medical imaging. For this, we study the class of large-scale pre-trained networks presented by Kolesnikov et al. on three diverse imaging tasks: chest radiography, mammography, and dermatology. We study both transfer performance and critical properties for the deployment in the medical domain, including: out-of-distribution generalization, data-efficiency, sub-group fairness, and uncertainty estimation. Interestingly, we find that for some of these properties transfer from natural to medical images is indeed extremely effective, but only when performed at sufficient scale.},
	archiveprefix = {arXiv},
	arxivid      = {2101.05913},
	eprint       = {2101.05913}
}
@inproceedings{lin2016scribblesup,
	title        = {{Scribblesup: Scribble-supervised convolutional networks for semantic segmentation}},
	author       = {Lin, Di and Dai, Jifeng and Jia, Jiaya and He, Kaiming and Sun, Jian},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {3159--3167}
}
@inproceedings{zamir2018taskonomy,
	title        = {{Taskonomy: Disentangling Task Transfer Learning}},
	author       = {Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
	year         = 2018,
	month        = {jun},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {3712--3722},
	url          = {http://taskonomy.vision/},
	abstract     = {Do visual tasks have a relationship, or are they unre-lated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values ; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. We proposes a fully computational approach for model-ing the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2 3 (compared to training independently) while keeping the performance nearly the same. We provide a set of tools for computing and probing this taxo-nomical structure including a solver that users can employ to devise efficient supervision policies for their use cases.},
	archiveprefix = {arXiv},
	arxivid      = {1804.08328v1},
	eprint       = {1804.08328v1},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Zamir et al. - 2018 - Taskonomy Disentangling Task Transfer Learning.pdf:pdf}
}
@inproceedings{siam2017deep,
	title        = {{Deep semantic segmentation for automated driving: Taxonomy, roadmap and challenges}},
	author       = {Siam, Mennatullah and Elkerdawy, Sara and Jagersand, Martin and Yogamani, Senthil},
	year         = 2017,
	booktitle    = {2017 IEEE 20th international conference on intelligent transportation systems (ITSC)},
	pages        = {1--8},
	organization = {IEEE}
}
@inproceedings{Adhikari2018,
	title        = {{Faster Bounding Box Annotation for Object Detection in Indoor Scenes}},
	author       = {Adhikari, Bishwo and Peltomaki, Jukka and Puura, Jussi and Huttunen, Heikki},
	year         = 2018,
	booktitle    = {2018 7th European Workshop on Visual Information Processing (EUVIP)},
	pages        = {1--6},
	doi          = {10.1109/EUVIP.2018.8611732}
}
@article{Liang2020,
	title        = {{A transfer learning method with deep residual network for pediatric pneumonia diagnosis}},
	author       = {Liang, Gaobo and Zheng, Lixin},
	year         = 2020,
	month        = {apr},
	journal      = {Computer Methods and Programs in Biomedicine},
	publisher    = {Elsevier},
	volume       = 187,
	pages        = 104964,
	doi          = {10.1016/J.CMPB.2019.06.023},
	issn         = {0169-2607},
	abstract     = {Background and Objective: Computer aided diagnosis systems based on deep learning and medical imaging is increasingly becoming research hotspots. At the moment, the classical convolutional neural network generates classification results by hierarchically abstracting the original image. These abstract features are less sensitive to the position and orientation of the object, and this lack of spatial information limits the further improvement of image classification accuracy. Therefore, how to develop a suitable neural network framework and training strategy in practical clinical applications to avoid this problem is a topic that researchers need to continue to explore. Methods: We propose a deep learning framework that combines residual thought and dilated convolution to diagnose and detect childhood pneumonia. Specifically, based on an understanding of the nature of the child pneumonia image classification task, the proposed method uses the residual structure to overcome the over-fitting and the degradation problems of the depth model, and utilizes dilated convolution to overcome the problem of loss of feature space information caused by the increment in depth of the model. Furthermore, in order to overcome the problem of difficulty in training model due to insufficient data and the negative impact of the introduction of structured noise on the performance of the model, we use the model parameters learned on large-scale datasets in the same field to initialize our model through transfer learning. Results: Our proposed method has been evaluated for extracting texture features associated with pneumonia and for accurately identifying the performance of areas of the image that best indicate pneumonia. The experimental results of the test dataset show that the recall rate of the method on children pneumonia classification task is 96.7{\%}, and the f1-score is 92.7{\%}. Compared with the prior art methods, this approach can effectively solve the problem of low image resolution and partial occlusion of the inflammatory area in children chest X-ray images. Conclusions: The novel framework focuses on the application of advanced classification that directly performs lesion characterization, and has high reliability in the classification task of children pneumonia.},
	annote       = {- Not pretrained on imagenet, but chestxray},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Liang, Zheng - 2020 - A transfer learning method with deep residual network for pediatric pneumonia diagnosis.pdf:pdf},
	keywords     = {Deep learning,Image classification,Pneumonia,Residual network,Transfer Learning},
	pmid         = 31262537
}
@article{jones1998efficient,
	title        = {{Efficient global optimization of expensive black-box functions}},
	author       = {Jones, Donald R and Schonlau, Matthias and Welch, William J},
	year         = 1998,
	journal      = {Journal of Global optimization},
	publisher    = {Springer},
	volume       = 13,
	number       = 4,
	pages        = {455--492}
}
@inproceedings{siddiqui2020viewal,
	title        = {{Viewal: Active learning with viewpoint entropy for semantic segmentation}},
	author       = {Siddiqui, Yawar and Valentin, Julien and Nie{\ss}ner, Matthias},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {9433--9443}
}
@inproceedings{ahn2018learning,
	title        = {{Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation}},
	author       = {Ahn, Jiwoon and Kwak, Suha},
	year         = 2018,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {4981--4990}
}
@article{Mockus1978,
	title        = {{The Application of Bayesian Methods for Seeking the Extremum}},
	author       = {Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
	year         = 1978,
	journal      = {Towards Global Optimization},
	publisher    = {Amsterdam: Elsevier},
	volume       = 2,
	number       = {117-129},
	pages        = 2
}
@inproceedings{tang2018normalized,
	title        = {{Normalized cut loss for weakly-supervised cnn segmentation}},
	author       = {Tang, Meng and Djelouah, Abdelaziz and Perazzi, Federico and Boykov, Yuri and Schroers, Christopher},
	year         = 2018,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {1818--1827}
}
@inproceedings{koleshnikov2021,
	title        = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
	author       = {Kolesnikov, Alexander and Dosovitskiy, Alexey and Weissenborn, Dirk and Heigold, Georg and Uszkoreit, Jakob and Beyer, Lucas and Minderer, Matthias and Dehghani, Mostafa and Houlsby, Neil and Gelly, Sylvain and Unterthiner, Thomas and Zhai, Xiaohua},
	year         = 2021
}
@article{2019LiTS,
	title        = {{The Liver Tumor Segmentation Benchmark (LiTS)}},
	author       = {Bilic, Patrick and Christ, Patrick Ferdinand and Vorontsov, Eugene and Chlebus, Grzegorz and Chen, Hao and Dou, Qi and Fu, Chi-Wing and Han, Xiao and Heng, Pheng-Ann and Hesser, J{\"{u}}rgen and Kadoury, Samuel and Konopczynski, Tomasz and Le, Miao and Li, Chunming and Li, Xiaomeng and Lipkov{\`{a}}, Jana and Lowengrub, John and Meine, Hans and {Hendrik Moltz}, Jan and Pal, Chris and Piraud, Marie and Qi, Xiaojuan and Qi, Jin and Rempfler, Markus and Roth, Karsten and Schenk, Andrea and Sekuboyina, Anjany and Vorontsov, Eugene and Zhou, Ping and H{\"{u}}lsemeyer, Christian and Beetz, Marcel and Ettlinger, Florian and Gruen, Felix and Kaissis, Georgios and Loh{\"{o}}fer, Fabian and Braren, Rickmer and Holch, Julian and Hofmann, Felix and Sommer, Wieland and Heinemann, Volker and Jacobs, Colin and {Efrain Humpire Mamani}, Gabriel and van Ginneken, Bram and Chartrand, Gabriel and Tang, An and Drozdzal, Michal and Ben-Cohen, Avi and Klang, Eyal and Amitai, Marianne M and Konen, Eli and Greenspan, Hayit and Moreau, Johan and Hostettler, Alexandre and Soler, Luc and Vivanti, Refael and Szeskin, Adi and Lev-Cohain, Naama and Sosna, Jacob and Joskowicz, Leo and Menze, Bjoern H},
	year         = 2019,
	month        = {jan},
	journal      = {arXiv e-prints},
	pages        = {arXiv:1901.04056},
	archiveprefix = {arXiv},
	arxivid      = {cs.CV/1901.04056},
	eprint       = {1901.04056},
	keywords     = {Computer Science - Computer Vision and Pattern Rec},
	primaryclass = {cs.CV}
}
@inproceedings{howard2019searching,
	title        = {{Searching for mobilenetv3}},
	author       = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Others},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {1314--1324}
}
@article{hesamian2019deep,
	title        = {{Deep learning techniques for medical image segmentation: achievements and challenges}},
	author       = {Hesamian, Mohammad Hesam and Jia, Wenjing and He, Xiangjian and Kennedy, Paul},
	year         = 2019,
	journal      = {Journal of digital imaging},
	publisher    = {Springer},
	volume       = 32,
	number       = 4,
	pages        = {582--596}
}
@inproceedings{zhou2016learning,
	title        = {{Learning deep features for discriminative localization}},
	author       = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {2921--2929}
}
@article{Simpson2019,
	title        = {{A large annotated medical image dataset for the development and evaluation of segmentation algorithms}},
	author       = {Simpson, Amber L and Antonelli, Michela and Bakas, Spyridon and Bilello, Michel and Farahani, Keyvan and {Van Ginneken}, Bram and Kopp-Schneider, Annette and Landman, Bennett A and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M and Bilic, Patrick and Christ, Patrick F and Do, Richard K G and Gollub, Marc and Golia-Pernicka, Jennifer and Heckers, Stephan H and Jarnagin, William R and Mchugo, Maureen K and Napel, Sandy and Vorontsov, Eugene and Maier-Hein, Lena and Cardoso, M Jorge},
	year         = 2019,
	journal      = {Memorial Sloan Kettering Cancer Center},
	volume       = 12,
	number       = 9,
	abstract     = {Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts.},
	archiveprefix = {arXiv},
	arxivid      = {1902.09063v1},
	eprint       = {1902.09063v1},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Simpson et al. - 2019 - Google DeepMind. 10. Imaging Biomarkers and Computer-aided Diagnosis Lab, Radiology and Imag-ing Sciences, Natio.pdf:pdf}
}
@article{Bodenstedt2018,
	title        = {Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery},
	author       = {Sebastian Bodenstedt and Max Allan and Anthony Agustinos and Xiaofei Du and Luis C. Garc{\'{\i}}a{-}Peraza{-}Herrera and Hannes Kenngott and Thomas Kurmann and Beat P. M{\"{u}}ller{-}Stich and S{\'{e}}bastien Ourselin and Daniil Pakhomov and Raphael Sznitman and Marvin Teichmann and Martin Thoma and Tom Vercauteren and Sandrine Voros and Martin Wagner and Pamela Wochner and Lena Maier{-}Hein and Danail Stoyanov and Stefanie Speidel},
	year         = 2018,
	journal      = {ArXiv},
	volume       = {abs/1805.02475},
	eprinttype   = {arXiv},
	eprint       = {1805.02475}
}
@article{chen2017rethinking,
	title        = {{Rethinking atrous convolution for semantic image segmentation}},
	author       = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1706.05587}
}
@inproceedings{Ranganathan2017,
	title        = {{Deep active learning for image classification}},
	author       = {Ranganathan, Hiranmayi and Venkateswara, Hemanth and Chakraborty, Shayok and Panchanathan, Sethuraman},
	year         = 2017,
	booktitle    = {2017 IEEE International Conference on Image Processing (ICIP)},
	pages        = {3934--3938},
	doi          = {10.1109/ICIP.2017.8297020}
}
@article{Benenson2019LargeScaleIO,
	title        = {{Large-Scale Interactive Object Segmentation With Human Annotators}},
	author       = {Benenson, Rodrigo and Popov, Stefan and Ferrari, Vittorio},
	year         = 2019,
	journal      = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {11692--11701}
}
@inproceedings{he2019,
	title        = {{Rethinking imagenet pre-training}},
	author       = {He, Kaiming and Girshick, Ross and Doll{\'{a}}r, Piotr},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {4918--4927},
	annote       = {- Imagenet pretraining doesn't help, it's the same as random init but it takes less time}
}
@inproceedings{song2019box,
	title        = {{Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation}},
	author       = {Song, Chunfeng and Huang, Yan and Ouyang, Wanli and Wang, Liang},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {3136--3145}
}
@article{kushner1964,
	title        = {{A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise}},
	author       = {Kushner, H J},
	year         = 1964,
	journal      = {Journal of Basic Engineering},
	volume       = 86,
	number       = 1,
	pages        = {97--106},
	doi          = {10.1115/1.3653121},
	issn         = {0021-9223},
	url          = {https://doi.org/10.1115/1.3653121},
	abstract     = {A versatile and practical method of searching a parameter space is presented. Theoretical and experimental results illustrate the usefulness of the method for such problems as the experimental optimization of the performance of a system with a very general multipeak performance function when the only available information is noise-distributed samples of the function. At present, its usefulness is restricted to optimization with respect to one system parameter. The observations are taken sequentially; but, as opposed to the gradient method, the observation may be located anywhere on the parameter interval. A sequence of estimates of the location of the curve maximum is generated. The location of the next observation may be interpreted as the location of the most likely competitor (with the current best estimate) for the location of the curve maximum. A Brownian motion stochastic process is selected as a model for the unknown function, and the observations are interpreted with respect to the model. The model gives the results a simple intuitive interpretation and allows the use of simple but efficient sampling procedures. The resulting process possesses some powerful convergence properties in the presence of noise; it is nonparametric and, despite its generality, is efficient in the use of observations. The approach seems quite promising as a solution to many of the problems of experimental system optimization.}
}
@article{Esteva2017,
	title        = {{Dermatologist-level classification of skin cancer with deep neural networks}},
	author       = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
	year         = 2017,
	journal      = {Nature},
	volume       = 542,
	number       = 7639,
	pages        = {115--118},
	doi          = {10.1038/nature21056},
	issn         = {1476-4687},
	url          = {https://doi.org/10.1038/nature21056},
	abstract     = {An artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers achieves the accuracy of board-certified dermatologists.}
}
@inproceedings{cordts2016cityscapes,
	title        = {{The cityscapes dataset for semantic urban scene understanding}},
	author       = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {3213--3223}
}
@article{Zhang2021,
	title        = {{Affinity Attention Graph Neural Network for Weakly Supervised Semantic Segmentation}},
	author       = {Zhang, Bingfeng and Xiao, Jimin and Jiao, Jianbo and Wei, Yunchao and Zhao, Yao},
	year         = 2021,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages        = {1--1},
	doi          = {10.1109/TPAMI.2021.3083269},
	issn         = {0162-8828},
	url          = {https://github.com/zbf1991/A2GNN. https://ieeexplore.ieee.org/document/9440699/},
	abstract     = {Weakly supervised semantic segmentation is receiving great attention due to its low human annotation cost. In this paper, we aim to tackle bounding box supervised semantic segmentation, i.e., training accurate semantic segmentation models using bounding box annotations as supervision. To this end, we propose Affinity Attention Graph Neural Network (A 2 GNN). Following previous practices, we first generate pseudo semantic-aware seeds, which are then formed into semantic graphs based on our newly proposed affinity Convolutional Neural Network (CNN). Then the built graphs are input to our A 2 GNN, in which an affinity attention layer is designed to acquire the short-and long-distance information from soft graph edges to accurately propagate semantic labels from the confident seeds to the unlabeled pixels. However, to guarantee the precision of the seeds, we only adopt a limited number of confident pixel seed labels for A 2 GNN, which may lead to insufficient supervision for training. To alleviate this issue, we further introduce a new loss function and a consistency-checking mechanism to leverage the bounding box constraint, so that more reliable guidance can be included for the model optimization. Experiments show that our approach achieves new state-of-the-art performances on Pascal VOC 2012 datasets (val: 76.5{\%}, test: 75.2{\%}). More importantly, our approach can be readily applied to bounding box supervised instance segmentation task or other weakly supervised semantic segmentation tasks, with state-of-the-art or comparable performance among almot all weakly supervised tasks on PASCAL VOC or COCO dataset. Our source code will be available at https://github.com/zbf1991/A2GNN.},
	archiveprefix = {arXiv},
	arxivid      = {2106.04054v1},
	eprint       = {2106.04054v1},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Affinity Attention Graph Neural Network for Weakly Supervised Semantic Segmentation.pdf:pdf},
	keywords     = {Index Terms-Weakly supervised,graph neural network,semantic segmentation}
}
@inproceedings{papadopoulos2014,
	title        = {{Training Object Class Detectors from Eye Tracking Data}},
	author       = {Papadopoulos, Dim P and Clarke, Alasdair D F and Keller, Frank and Ferrari, Vittorio},
	year         = 2014,
	booktitle    = {Computer Vision -- ECCV 2014},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {361--376},
	isbn         = {978-3-319-10602-1},
	abstract     = {Training an object class detector typically requires a large set of images annotated with bounding-boxes, which is expensive and time consuming to create. We propose novel approach to annotate object locations which can substantially reduce annotation time. We first track the eye movements of annotators instructed to find the object and then propose a technique for deriving object bounding-boxes from these fixations. To validate our idea, we collected eye tracking data for the trainval part of 10 object classes of Pascal VOC 2012 (6,270 images, 5 observers). Our technique correctly produces bounding-boxes in 50{\%}of the images, while reducing the total annotation time by factor 6.8{\{}$\backslash$texttimes{\}} compared to drawing bounding-boxes. Any standard object class detector can be trained on the bounding-boxes predicted by our model. Our large scale eye tracking dataset is available at groups.inf.ed.ac.uk/calvin/eyetrackdataset/.},
	editor       = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne}
}
@book{Rasmussen2006,
	title        = {{Gaussian Processes for Machine Learning}},
	author       = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year         = 2006,
	publisher    = {The MIT Press},
	pages        = 266,
	isbn         = {026218253X},
	url          = {www.GaussianProcess.org/gpml},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Rasmussen, Williams - Unknown - Gaussian Processes for Machine Learning.pdf:pdf}
}
@inproceedings{pmlr-v102-geyer19a,
	title        = {{Transfer Learning by Adaptive Merging of Multiple Models}},
	author       = {Geyer, Robin and Corinzia, Luca and Wegmayr, Viktor},
	year         = 2019,
	booktitle    = {Proceedings of The 2nd International Conference on Medical Imaging with Deep Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 102,
	pages        = {185--196},
	url          = {https://proceedings.mlr.press/v102/geyer19a.html},
	abstract     = {Transfer learning has been an important ingredient of state-of-the-art deep learning models. In particular, it has significant impact when little data is available for the target task, such as in many medical imaging applications. Typically, transfer learning means pre-training the target model on a related task which has sufficient data available. However, often pre-trained models from several related tasks are available, and it would be desirable to transfer their combined knowledge by automatic weighting and merging. For this reason, we propose T-IMM (Transfer Incremental Mode Matching), a method to leverage several pre-trained models, which extends the concept of Incremental Mode Matching from lifelong learning to the transfer learning setting. Our method introduces layer wise mixing ratios, which are learned automatically and fuse multiple pre-trained models before fine-tuning on the new task. We demonstrate the efficacy of our method by the example of brain tumor segmentation in MRI (BRATS 2018 Challange). We show that fusing weights according to our framework, merging two models trained on general brain parcellation can greatly enhance the final model performance for small training sets when compared to standard transfer methods or state-of the art initialization. We further demonstrate that the benefit remains even when training on the entire Brats 2018 data set (255 patients).},
	annote       = {- Transfer learning with a different thing, not imagenet},
	editor       = {Cardoso, M Jorge and Feragen, Aasa and Glocker, Ben and Konukoglu, Ender and Oguz, Ipek and Unal, Gozde and Vercauteren, Tom}
}
@incollection{mccloskey1989catastrophic,
	title        = {{Catastrophic interference in connectionist networks: The sequential learning problem}},
	author       = {McCloskey, Michael and Cohen, Neal J},
	year         = 1989,
	booktitle    = {Psychology of learning and motivation},
	publisher    = {Elsevier},
	volume       = 24,
	pages        = {109--165}
}
@article{chen2018priors,
	title        = {{How priors of initial hyperparameters affect Gaussian process regression models}},
	author       = {Chen, Zexun and Wang, Bo},
	year         = 2018,
	journal      = {Neurocomputing},
	publisher    = {Elsevier},
	volume       = 275,
	pages        = {1702--1710}
}
@inproceedings{deng2009imagenet,
	title        = {{Imagenet: A large-scale hierarchical image database}},
	author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year         = 2009,
	booktitle    = {2009 IEEE conference on computer vision and pattern recognition},
	pages        = {248--255},
	organization = {Ieee}
}
@incollection{raghu2019transfusion,
	title        = {{Transfusion: Understanding Transfer Learning for Medical Imaging}},
	author       = {Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
	year         = 2019,
	journal      = {Advances in neural information processing systems},
	booktitle    = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
	publisher    = {Curran Associates Inc.},
	volume       = 32,
	url          = {https://papers.nips.cc/paper/2019/hash/eb1e78328c46506b46a4ac4a1e378b91-Abstract.html},
	abstract     = {Transfer learning from natural image datasets, particularly IMAGENET, using standard large models and corresponding pretrained weights has become a de-facto method for deep learning applications to medical imaging. However, there are fundamental differences in data sizes, features and task specifications between natural image classification and the target medical tasks, and there is little understanding of the effects of transfer. In this paper, we explore properties of transfer learning for medical imaging. A performance evaluation on two large scale medical imaging tasks shows that surprisingly, transfer offers little benefit to performance, and simple, lightweight models can perform comparably to IMAGENET architectures. Investigating the learned representations and features, we find that some of the differences from transfer learning are due to the over-parametrization of standard models rather than sophisticated feature reuse. We isolate where useful feature reuse occurs, and outline the implications for more efficient model exploration. We also explore feature independent benefits of transfer arising from weight scalings.},
	annote       = {- Transfer learning offers limited performance gains in small models},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Raghu et al. - Unknown - Transfusion Understanding Transfer Learning for Medical Imaging.pdf:pdf}
}
@inproceedings{kolesnikov2020big,
	title        = {{Big transfer (bit): General visual representation learning}},
	author       = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
	year         = 2020,
	booktitle    = {European conference on computer vision},
	pages        = {491--507},
	organization = {Springer}
}
@techreport{WelinderEtal2010,
	title        = {{Caltech-UCSD Birds 200}},
	author       = {Welinder, P and Branson, S and Mita, T and Wah, C and Schroff, F and Belongie, S and Perona, P},
	year         = 2010,
	number       = {CNS-TR-2010-001},
	institution  = {California Institute of Technology}
}
@inproceedings{sun2017,
	title        = {{Revisiting unreasonable effectiveness of data in deep learning era}},
	author       = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {843--852}
}
@article{Zhai2021,
	title        = {{Scaling Vision Transformers}},
	author       = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
	year         = 2021,
	month        = {jun},
	journal      = {arXiv preprint arXiv:2106.04560},
	url          = {https://arxiv.org/abs/2106.04560v1},
	abstract     = {Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45{\%} top-1 accuracy. The model also performs well on few-shot learning, for example, attaining 84.86{\%} top-1 accuracy on ImageNet with only 10 examples per class.},
	archiveprefix = {arXiv},
	arxivid      = {2106.04560},
	eprint       = {2106.04560},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Zhai et al. - 2021 - Scaling Vision Transformers.pdf:pdf}
}
@misc{pascal-voc-2012,
	title        = {{The PASCAL Visual Object Classes Challenge 2012 VOC2012 Results}},
	author       = {Everingham, M and Van Gool, L and Williams, C K I and Winn, J and Zisserman, A},
	howpublished = {http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html}
}
@article{heker2020joint,
	title        = {{Joint liver lesion segmentation and classification via transfer learning}},
	author       = {Heker, Michal and Greenspan, Hayit},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2004.12352},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Heker, Greenspan - Unknown - Medical Imaging with Deep Learning 2020 1-5 MIDL 2020-Short Paper Joint Liver Lesion Segmentation and Class.pdf:pdf}
}
@inproceedings{islam2020semantic,
	title        = {{Semantic segmentation of underwater imagery: Dataset and benchmark}},
	author       = {Islam, Md Jahidul and Edge, Chelsey and Xiao, Yuyang and Luo, Peigen and Mehtaz, Muntaqim and Morse, Christopher and Enan, Sadman Sakib and Sattar, Junaed},
	year         = 2020,
	booktitle    = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	pages        = {1769--1776},
	organization = {IEEE}
}
@inproceedings{Konyushkova_2018_CVPR,
	title        = {{Learning Intelligent Dialogs for Bounding Box Annotation}},
	author       = {Konyushkova, Ksenia and Uijlings, Jasper and Lampert, Christoph H and Ferrari, Vittorio},
	year         = 2018,
	month        = {6},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{lee2019ficklenet,
	title        = {{Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference}},
	author       = {Lee, Jungbeom and Kim, Eunji and Lee, Sungmin and Lee, Jangho and Yoon, Sungroh},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {5267--5276}
}
@article{Ding2019,
	title        = {{A Deep Learning Model to Predict a Diagnosis of Alzheimer Disease by Using 18F-FDG PET of the Brain}},
	author       = {Ding, Yiming and Sohn, Jae Ho and Kawczynski, Michael G and Trivedi, Hari and Harnish, Roy and Jenkins, Nathaniel W and Lituiev, Dmytro and Copeland, Timothy P and Aboian, Mariam S and {Mari Aparici}, Carina and Behr, Spencer C and Flavell, Robert R and Huang, Shih-Ying and Zalocusky, Kelly A and Nardo, Lorenzo and Seo, Youngho and Hawkins, Randall A and {Hernandez Pampaloni}, Miguel and Hadley, Dexter and Franc, Benjamin L},
	year         = 2019,
	journal      = {Radiology},
	volume       = 290,
	number       = 2,
	pages        = {456--464},
	doi          = {10.1148/radiol.2018180958},
	url          = {https://doi.org/10.1148/radiol.2018180958},
	abstract     = {Purpose To develop and validate a deep learning algorithm that predicts the final diagnosis of Alzheimer disease (AD), mild cognitive impairment, or neither at fluorine 18 (18F) fluorodeoxyglucose (FDG) PET of the brain and compare its performance to that of radiologic readers. Materials and Methods Prospective 18F-FDG PET brain images from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (2109 imaging studies from 2005 to 2017, 1002 patients) and retrospective independent test set (40 imaging studies from 2006 to 2016, 40 patients) were collected. Final clinical diagnosis at follow-up was recorded. Convolutional neural network of InceptionV3 architecture was trained on 90{\%} of ADNI data set and tested on the remaining 10{\%}, as well as the independent test set, with performance compared to radiologic readers. Model was analyzed with sensitivity, specificity, receiver operating characteristic (ROC), saliency map, and t-distributed stochastic neighbor embedding. Results The algorithm achieved area under the ROC curve of 0.98 (95{\%} confidence interval: 0.94, 1.00) when evaluated on predicting the final clinical diagnosis of AD in the independent test set (82{\%} specificity at 100{\%} sensitivity), an average of 75.8 months prior to the final diagnosis, which in ROC space outperformed reader performance (57{\%} [four of seven] sensitivity, 91{\%} [30 of 33] specificity; P {\textless} .05). Saliency map demonstrated attention to known areas of interest but with focus on the entire brain. Conclusion By using fluorine 18 fluorodeoxyglucose PET of the brain, a deep learning algorithm developed for early prediction of Alzheimer disease achieved 82{\%} specificity at 100{\%} sensitivity, an average of 75.8 months prior to the final diagnosis. {\textcopyright} RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Larvie in this issue.},
	annote       = {PMID: 30398430}
}
@inproceedings{li2020mas3k,
	title        = {{MAS3K: An Open Dataset for Marine Animal Segmentation}},
	author       = {Li, Lin and Rigall, Eric and Dong, Junyu and Chen, Geng},
	year         = 2020,
	booktitle    = {International Symposium on Benchmarking, Measuring and Optimization},
	pages        = {194--212},
	organization = {Springer}
}
@article{Ki2021,
	title        = {{Contrastive and consistent feature learning for weakly supervised object localization and semantic segmentation}},
	author       = {Ki, Minsong and Uh, Youngjung and Lee, Wonyoung and Byun, Hyeran},
	year         = 2021,
	month        = {jul},
	journal      = {Neurocomputing},
	publisher    = {Elsevier B.V.},
	volume       = 445,
	pages        = {244--254},
	doi          = {10.1016/j.neucom.2021.03.023},
	issn         = 18728286,
	url          = {https://doi.org/10.1016/j.neucom.2021.03.023},
	abstract     = {Weakly supervised learning attempts to construct predictive models by learning with weak supervision. In this paper, we concentrate on weakly supervised object localization and semantic segmentation tasks. Existing methods are limited to focusing on narrow discriminative parts or overextending the activations to less discriminative regions even on backgrounds. To mitigate these problems, we regard the background as an important cue that guides the feature activation to cover the entire object to the right extent, and propose two novel objective functions: 1) contrastive attention loss and 2) foreground consistency loss. Contrastive attention loss draws the foreground feature and its dropped version close together and pushes the dropped foreground feature away from the background feature. Foreground consistency loss favors agreement between layers and provides early layers with a sense of objectness. Using both losses leads to balanced improvements over localization and segmentation accuracy by boosting activations on less discriminative regions but restraining the activation in the target object extent. For better optimizing the above losses, we use the non-local attention blocks to replace channel-pooled attention leading to enhanced attention maps considering the spatial similarity. Finally, our method achieves state-of-the-art localization performance on CUB-200-2011, ImageNet, and OpenImages benchmarks regarding top-1 localization accuracy, MaxBoxAccV2, and PxAP. We also demonstrate the effectiveness of our method in improving segmentation performance measured by mIoU on the PASCAL VOC dataset.},
	annote       = {They get predictions in 41x41, then upsample and refine with conditional random fields.},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Ki et al. - 2021 - Contrastive and consistent feature learning for weakly supervised object localization and semantic segmentation.pdf:pdf},
	keywords     = {Contrastive learning,Foreground consistency,Localization,Segmentation,Weakly supervised learning}
}
@inproceedings{sless2019road,
	title        = {{Road scene understanding by occupancy grid learning from sparse radar clusters using semantic segmentation}},
	author       = {Sless, Liat and {El Shlomo}, Bat and Cohen, Gilad and Oron, Shaul},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
	pages        = {0}
}
@article{Mensink,
	title        = {{Factors of Influence for Transfer Learning across Diverse Appearance Domains and Task Types}},
	author       = {Mensink, Thomas and Uijlings, Jasper and Kuznetsova, Alina and Gygli, Michael and Ferrari, Vittorio},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2103.13318},
	abstract     = {Transfer learning enables to re-use knowledge learned on a source task to help learning a target task. A simple form of transfer learning is common in current state-of-the-art computer vision models, i.e. pre-training a model for image classification on the ILSVRC dataset, and then fine-tune on any target task. However, previous systematic studies of transfer learning have been limited and the circumstances in which it is expected to work are not fully understood. In this paper we carry out an extensive experimental exploration of transfer learning across vastly different image domains (consumer photos, autonomous driving, aerial imagery, underwater, indoor scenes, synthetic, close-ups) and task types (semantic segmentation, object detection, depth estimation, keypoint detection). Importantly, these are all complex, structured output tasks types relevant to modern computer vision applications. In total we carry out over 1200 transfer experiments, including many where the source and target come from different image domains, task types, or both. We systematically analyze these experiments to understand the impact of image domain, task type, and dataset size on transfer learning performance. Our study leads to several insights and concrete recommendations for practitioners.},
	annote       = {In general, accross datasets, segmentation helps object detection but not vice versa. -ï»¿Transfer learning effects are larger for small target training sets. -ï»¿The source domain including the target is more important than the number of source samples.},
	archiveprefix = {arXiv},
	arxivid      = {2103.13318v1},
	eprint       = {2103.13318v1},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Mensink et al. - Unknown - Factors of Influence for Transfer Learning across Diverse Appearance Domains and Task Types.pdf:pdf},
	keywords     = {Computer Vision,Index Terms-Transfer Learning}
}
@inproceedings{menegola2017,
	title        = {{Knowledge transfer for melanoma screening with deep learning}},
	author       = {Menegola, Afonso and Fornaciali, Michel and Pires, Ramon and Bittencourt, Fl{\'{a}}via Vasques and Avila, Sandra and Valle, Eduardo},
	year         = 2017,
	booktitle    = {2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)},
	pages        = {297--300},
	annote       = {Imagenet pretraining},
	organization = {IEEE}
}
@article{swersky2013multi,
	title        = {{Multi-task bayesian optimization}},
	author       = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P},
	year         = 2013,
	journal      = {Advances in neural information processing systems},
	volume       = 26
}
@inproceedings{andriluka2018fluid,
	title        = {{Fluid annotation: a human-machine collaboration interface for full image annotation}},
	author       = {Andriluka, Mykhaylo and Uijlings, Jasper R R and Ferrari, Vittorio},
	year         = 2018,
	booktitle    = {Proceedings of the 26th ACM international conference on Multimedia},
	pages        = {1957--1966}
}
@inproceedings{wang2017chestxray,
	title        = {{ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases}},
	author       = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald},
	year         = 2017,
	booktitle    = {2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)},
	pages        = {3462--3471},
	annote       = {ChestX-ray14 dataset}
}
@article{tseng2021person,
	title        = {{Person Retrieval in Video Surveillance Using Deep Learning--Based Instance Segmentation}},
	author       = {Tseng, Chien-Hao and Hsieh, Chia-Chien and Jwo, Dah-Jing and Wu, Jyh-Horng and Sheu, Ruey-Kai and Chen, Lun-Chi},
	year         = 2021,
	journal      = {Journal of Sensors},
	publisher    = {Hindawi},
	volume       = 2021
}
@inproceedings{jiang2019integral,
	title        = {{Integral object mining via online attention accumulation}},
	author       = {Jiang, Peng-Tao and Hou, Qibin and Cao, Yang and Cheng, Ming-Ming and Wei, Yunchao and Xiong, Hong-Kai},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {2070--2079}
}
@article{Bergstra2012,
	title        = {{Random search for hyper-parameter optimization.}},
	author       = {Bergstra, James and Bengio, Yoshua},
	year         = 2012,
	journal      = {Journal of machine learning research},
	volume       = 13,
	number       = 2
}
@inproceedings{papadopoulos2017extreme,
	title        = {{Extreme clicking for efficient object annotation}},
	author       = {Papadopoulos, Dim P and Uijlings, Jasper R R and Keller, Frank and Ferrari, Vittorio},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {4930--4939}
}
@inproceedings{domhan2015speeding,
	title        = {{Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves}},
	author       = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
	year         = 2015,
	booktitle    = {Twenty-fourth international joint conference on artificial intelligence}
}
@inproceedings{ahn2019weakly,
	title        = {{Weakly supervised learning of instance segmentation with inter-pixel relations}},
	author       = {Ahn, Jiwoon and Cho, Sunghyun and Kwak, Suha},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {2209--2218}
}
@inproceedings{ronneberger2015u,
	title        = {{U-net: Convolutional networks for biomedical image segmentation}},
	author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year         = 2015,
	booktitle    = {International Conference on Medical image computing and computer-assisted intervention},
	pages        = {234--241},
	organization = {Springer}
}
@article{liu2020fsd,
	title        = {{FSD-10: a dataset for competitive sports content analysis}},
	author       = {Liu, Shenlan and Liu, Xiang and Huang, Gao and Feng, Lin and Hu, Lianyu and Jiang, Dong and Zhang, Aibin and Liu, Yang and Qiao, Hong},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2002.03312}
}
@article{Tajbakhsh2016,
	title        = {{Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?}},
	author       = {Tajbakhsh, Nima and Shin, Jae Y. and Gurudu, Suryakanth R. and Hurst, R. Todd and Kendall, Christopher B. and Gotway, Michael B. and Liang, Jianming},
	year         = 2016,
	month        = {may},
	journal      = {IEEE Transactions on Medical Imaging},
	publisher    = {Institute of Electrical and Electronics Engineers Inc.},
	volume       = 35,
	number       = 5,
	pages        = {1299--1312},
	doi          = {10.1109/TMI.2016.2535302},
	issn         = {1558254X},
	abstract     = {Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.},
	archiveprefix = {arXiv},
	arxivid      = {1706.00712},
	eprint       = {1706.00712},
	file         = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Tajbakhsh et al. - 2016 - Convolutional Neural Networks for Medical Image Analysis Full Training or Fine Tuning.pdf:pdf},
	keywords     = {Carotid intima-media thickness,computer-aided detection,convolutional neural networks,deep learning,fine-tuning,medical image analysis,polyp detection,pulmonary embolism detection,video quality assessment},
	pmid         = 26978662
}
@inproceedings{Zhang2016,
	title        = {{Function Extrapolation at One Inaccessible Point Using Converging Lines}},
	author       = {Zhang, Yiming and Kim, Nam Ho and Park, Chanyoung and Haftka, Raphael T.},
	year         = 2015,
	month        = {aug},
	booktitle    = {Volume 2B: 41st Design Automation Conference},
	publisher    = {American Society of Mechanical Engineers},
	doi          = {10.1115/DETC2015-47689},
	isbn         = {978-0-7918-5708-3},
	url          = {https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings/IDETC-CIE2015/57083/Boston, Massachusetts, USA/251314},
	abstract     = {Focus of this paper is on the prediction accuracy of multidimensional functions at an inaccessible point. The paper explores the possibility of extrapolating a high-dimensional function using multiple one-dimensional converging lines. The main idea is to select samples along lines towards the inaccessible point. Multi-dimensional extrapolation is thus transformed into a series of one-dimensional extrapolations that provide multiple estimates at the inaccessible point. We demonstrate the performance of converging lines using Kriging to extrapolate a two-dimensional drag coefficient function. Post-processing of extrapolation results from different lines based on Bayesian theory is proposed to combine the multiple predictions. Selection of lines is also discussed. The method of converging lines proves to be more robust and reliable than two-dimensional Kriging surrogate for the example.}
}
@inproceedings{fan2020learning,
	title        = {{Learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation}},
	author       = {Fan, Junsong and Zhang, Zhaoxiang and Song, Chunfeng and Tan, Tieniu},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {4283--4292}
}
@article{snoek2012practical,
	title        = {{Practical bayesian optimization of machine learning algorithms}},
	author       = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	year         = 2012,
	journal      = {Advances in neural information processing systems},
	volume       = 25
}
@inproceedings{Joshi2009,
	title        = {{Multi-class active learning for image classification}},
	author       = {Joshi, Ajay J and Porikli, Fatih and Papanikolopoulos, Nikolaos},
	year         = 2009,
	booktitle    = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
	pages        = {2372--2379},
	doi          = {10.1109/CVPR.2009.5206627}
}
@inproceedings{Bearman16,
	title        = {What's the Point: Semantic Segmentation with Point Supervision},
	author       = {Bearman, Amy and Russakovsky, Olga and Ferrari, Vittorio and Fei-Fei, Li},
	year         = 2016,
	booktitle    = {Computer Vision -- ECCV 2016},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {549--565},
	isbn         = {978-3-319-46478-7},
	editor       = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	abstract     = {The semantic image segmentation task presents a trade-off between test time accuracy and training time annotation cost. Detailed per-pixel annotations enable training accurate models but are very time-consuming to obtain; image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We incorporate this point supervision along with a novel objectness potential in the training loss function of a CNN model. Experimental results on the PASCAL VOC 2012 benchmark reveal that the combined effect of point-level supervision and objectness potential yields an improvement of {\$}{\$}12.9{\backslash},{\backslash}{\%}{\$}{\$}mIOU over image-level supervision. Further, we demonstrate that models trained with point-level supervision are more accurate than models trained with image-level, squiggle-level or full supervision given a fixed annotation budget.}
}
@inproceedings{Papandreou15,
	title        = {Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation},
	author       = {Papandreou, George and Chen, Liang-Chieh and Murphy, Kevin P. and Yuille, Alan L.},
	year         = 2015,
	month        = {December},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@inproceedings{Casanova2020Reinforced,
	title        = {Reinforced active learning for image segmentation},
	author       = {Arantxa Casanova and Pedro O. Pinheiro and Negar Rostamzadeh and Christopher J. Pal},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=SkgC6TNFvr}
}
@inproceedings{Konyushkova15,
	title        = {Introducing Geometry in Active Learning for Image Segmentation},
	author       = {Konyushkova, Ksenia and Sznitman, Raphael and Fua, Pascal},
	year         = 2015,
	booktitle    = {2015 IEEE International Conference on Computer Vision (ICCV)},
	pages        = {2974--2982}
}
@inproceedings{Cai21,
	title        = {Revisiting Superpixels for Active Learning in Semantic Segmentation with Realistic Annotation Costs},
	author       = {Cai, Lile and Xu, Xun and Liew, Jun Hao and Sheng Foo, Chuan},
	year         = 2021,
	booktitle    = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {10983--10992}
}
@article{mahmood2022optimizing,
	title        = {Optimizing Data Collection for Machine Learning},
	author       = {Rafid Mahmood and James Lucas and Jose M. Alvarez and Sanja Fidler and Marc T. Law},
	year         = 2022,
	month        = 10,
	journal      = {Advances in Neural Information Processing Systems (NeurIPS)},
	url          = {http://arxiv.org/abs/2210.01234},
	abstract     = {Modern deep learning systems require huge data sets to achieve impressive performance, but there is little guidance on how much or what kind of data to collect. Over-collecting data incurs unnecessary present costs, while under-collecting may incur future costs and delay workflows. We propose a new paradigm for modeling the data collection workflow as a formal optimal data collection problem that allows designers to specify performance targets, collection costs, a time horizon, and penalties for failing to meet the targets. Additionally, this formulation generalizes to tasks requiring multiple data sources, such as labeled and unlabeled data used in semi-supervised learning. To solve our problem, we develop Learn-Optimize-Collect (LOC), which minimizes expected future collection costs. Finally, we numerically compare our framework to the conventional baseline of estimating data requirements by extrapolating from neural scaling laws. We significantly reduce the risks of failing to meet desired performance targets on several classification, segmentation, and detection tasks, while maintaining low total collection costs.}
}
@article{mahmood2022,
	title        = {How Much More Data Do I Need? Estimating Requirements for Downstream Tasks},
	author       = {Rafid Mahmood and James Lucas and David Acuna and Daiqing Li and Jonah Philion and Jose M Alvarez and Zhiding Yu and Sanja Fidler and Marc T Law},
	year         = 2022,
	journal      = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {275--284},
	url          = {https://openaccess.thecvf.com/content/CVPR2022/html/Mahmood_How_Much_More_Data_Do_I_Need_Estimating_Requirements_for_CVPR_2022_paper.html},
	abstract     = {Given a small training data set and a learning algorithm , how much more data is necessary to reach a target validation or test performance? This question is of critical importance in applications such as autonomous driving or medical imaging where collecting data is expensive and time-consuming. Overestimating or underestimating data requirements incurs substantial costs that could be avoided with an adequate budget. Prior work on neural scaling laws suggest that the power-law function can fit the validation performance curve and extrapolate it to larger data set sizes. We find that this does not immediately translate to the more difficult downstream task of estimating the required data set size to meet a target performance. In this work, we consider a broad class of computer vision tasks and systematically investigate a family of functions that generalize the power-law function to allow for better estimation of data requirements. Finally, we show that incorporating a tuned correction factor and collecting over multiple rounds significantly improves the performance of the data estimators. Using our guidelines, practitioners can accurately estimate data requirements of machine learning systems to gain savings in both development time and data acquisition costs.}
}
% journal
@article{leibetseder2022endometriosis,
	title        = {Endometriosis detection and localization in laparoscopic gynecology},
	author       = {Leibetseder, Andreas and Schoeffmann, Klaus and Keckstein, J{\"o}rg and Keckstein, Simon},
	year         = 2022,
	journal      = {Multimedia Tools and Applications},
	publisher    = {Springer},
	pages        = {1--25},
	doi          = {10.1007/s11042-021-11730-1},
	url          = {https://doi.org/10.1007/s11042-021-11730-1}
}
@article{CMES,
	title        = {Complications and Their Management in Endometriosis Surgery},
	author       = {Yucel Karaman and Husamettin Uslu},
	year         = 2015,
	journal      = {Women's Health},
	volume       = 11,
	number       = 5,
	pages        = {685--692},
	doi          = {10.2217/whe.15.55},
	url          = {https://doi.org/10.2217/whe.15.55},
	eprint       = {https://doi.org/10.2217/whe.15.55}
}
% demo
@inproceedings{DBLP:conf/cbmi/LeibetsederSKK21,
	title        = {Post-surgical Endometriosis Segmentation in Laparoscopic Videos},
	author       = {Andreas Leibetseder and Klaus Schoeffmann and J{\"{o}}rg Keckstein and Simon Keckstein},
	year         = 2021,
	booktitle    = {18th International Conference on Content-Based Multimedia Indexing, {CBMI} 2021, Lille, France, June 28-30, 2021},
	publisher    = {{IEEE}},
	pages        = {1--4},
	doi          = {10.1109/CBMI50038.2021.9461900},
	url          = {https://doi.org/10.1109/CBMI50038.2021.9461900},
	timestamp    = {Tue, 29 Jun 2021 17:37:02 +0200},
	biburl       = {https://dblp.org/rec/conf/cbmi/LeibetsederSKK21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%Nets
@inproceedings{SaE,
	title        = {Squeeze-and-Excitation Networks},
	author       = {J. {Hu} and L. {Shen} and G. {Sun}},
	year         = 2018,
	booktitle    = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	volume       = {},
	number       = {},
	pages        = {7132--7141},
	doi          = {10.1109/CVPR.2018.00745}
}
@article{UNet++,
	title        = {UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation},
	author       = {Z. {Zhou} and M. M. R. {Siddiquee} and N. {Tajbakhsh} and J. {Liang}},
	year         = 2020,
	journal      = {IEEE Transactions on Medical Imaging},
	volume       = 39,
	number       = 6,
	pages        = {1856--1867},
	doi          = {10.1109/TMI.2019.2959609}
}
@article{CPFNet,
	title        = {CPFNet: Context Pyramid Fusion Network for Medical Image Segmentation},
	author       = {Feng, Shuanglang and Zhao, Heming and Shi, Fei and Cheng, Xuena and Wang, Meng and Ma, Yuhui and Xiang, Dehui and Zhu, Weifang and Chen, Xinjian},
	year         = 2020,
	journal      = {IEEE Transactions on Medical Imaging},
	volume       = 39,
	number       = 10,
	pages        = {3008--3018},
	doi          = {10.1109/TMI.2020.2983721}
}
@article{CE-Net,
	title        = {CE-Net: Context Encoder Network for 2D Medical Image Segmentation},
	author       = {Z. {Gu} and J. {Cheng} and H. {Fu} and K. {Zhou} and H. {Hao} and Y. {Zhao} and T. {Zhang} and S. {Gao} and J. {Liu}},
	year         = 2019,
	journal      = {IEEE Transactions on Medical Imaging},
	volume       = 38,
	number       = 10,
	pages        = {2281--2292},
	doi          = {10.1109/TMI.2019.2903562}
}
@article{MultiResUNet,
	title        = {MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation},
	author       = {Nabil Ibtehaz and M. Sohel Rahman},
	year         = 2020,
	journal      = {Neural Networks},
	volume       = 121,
	pages        = {74--87},
	doi          = {https://doi.org/10.1016/j.neunet.2019.08.025},
	issn         = {0893-6080},
	url          = {https://www.sciencedirect.com/science/article/pii/S0893608019302503},
	keywords     = {Convolutional neural networks, Medical imaging, Semantic segmentation, U-Net}
}
@article{SCSE,
	title        = {Recalibrating Fully Convolutional Networks With Spatial and Channel âSqueeze and Excitationâ Blocks},
	author       = {Roy, Abhijit Guha and Navab, Nassir and Wachinger, Christian},
	year         = 2019,
	journal      = {IEEE Transactions on Medical Imaging},
	volume       = 38,
	number       = 2,
	pages        = {540--549},
	doi          = {10.1109/TMI.2018.2867261}
}
@inproceedings{LensID,
	title        = {LensID: A CNN-RNN-Based Framework Towards Lens Irregularity Detection in Cataract Surgery Videos},
	author       = {Ghamsarian, Negin and Taschwer, Mario and Putzgruber-Adamitsch, Doris and Sarny, Stephanie and El-Shabrawi, Yosuf and Schoeffmann, Klaus},
	year         = 2021,
	booktitle    = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2021},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {76--86},
	isbn         = {978-3-030-87237-3},
	editor       = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline}
}
@inproceedings{FED-Net,
	title        = {Feature Fusion Encoder Decoder Network for Automatic Liver Lesion Segmentation},
	author       = {X. {Chen} and R. {Zhang} and P. {Yan}},
	year         = 2019,
	booktitle    = {2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)},
	volume       = {},
	number       = {},
	pages        = {430--433},
	doi          = {10.1109/ISBI.2019.8759555}
}
@inproceedings{ReCal-Net,
	title        = {ReCal-Net: Joint Region-Channel-Wise Calibrated Network for Semantic Segmentation in Cataract Surgery Videos},
	author       = {Ghamsarian, Negin and Taschwer, Mario and Putzgruber-Adamitsch, Doris and Sarny, Stephanie and El-Shabrawi, Yosuf and Sch{\"o}ffmann, Klaus},
	year         = 2021,
	booktitle    = {Neural Information Processing},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {391--402},
	editor       = {Mantoro, Teddy and Lee, Minho and Ayu, Media Anugerah and Wong, Kok Wai and Hidayanto, Achmad Nizar}
}
% Test_time adaptation:
@inproceedings{UDACB,
	title        = {Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-training},
	author       = {Zou, Yang and Yu, Zhiding and Vijaya Kumar, B. V. K. and Wang, Jinsong},
	year         = 2018,
	booktitle    = {Computer Vision -- ECCV 2018},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {297--313},
	isbn         = {978-3-030-01219-9},
	editor       = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair}
}
@inproceedings{BLDA,
	title        = {Bidirectional Learning for Domain Adaptation of Semantic Segmentation},
	author       = {Li, Yunsheng and Yuan, Lu and Vasconcelos, Nuno},
	year         = 2019,
	booktitle    = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	volume       = {},
	number       = {},
	pages        = {6929--6938},
	doi          = {10.1109/CVPR.2019.00710}
}
@inproceedings{FTTA,
	title        = {Fully Test-Time Adaptation for Image Segmentation},
	author       = {Hu, Minhao and Song, Tao and Gu, Yujun and Luo, Xiangde and Chen, Jieneng and Chen, Yinan and Zhang, Ya and Zhang, Shaoting},
	year         = 2021,
	booktitle    = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2021},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {251--260},
	isbn         = {978-3-030-87199-4},
	editor       = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline}
}
@inproceedings{TTUDA,
	title        = {Test-Time Unsupervised Domain Adaptation},
	author       = {Varsavsky, Thomas and Orbes-Arteaga, Mauricio and Sudre, Carole H. and Graham, Mark S. and Nachev, Parashkev and Cardoso, M. Jorge},
	year         = 2020,
	booktitle    = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2020},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {428--436},
	isbn         = {978-3-030-59710-8},
	editor       = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo}
}
@inproceedings{DGJP,
	title        = {Domain Generalization by Solving Jigsaw Puzzles},
	author       = {Carlucci, Fabio Maria and D'Innocente, Antonio and Bucci, Silvia and Caputo, Barbara and Tommasi, Tatiana},
	year         = 2019,
	booktitle    = {CVPR}
}
@article{TESSL,
	title        = {Temporal Ensembling for Semi-Supervised Learning},
	author       = {Samuli Laine and Timo Aila},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1610.02242},
	url          = {http://arxiv.org/abs/1610.02242},
	eprinttype   = {arXiv},
	eprint       = {1610.02242},
	timestamp    = {Mon, 13 Aug 2018 16:46:11 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/LaineA16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{UDAMIS,
	title        = {Unsupervised domain adaptation for medical imaging segmentation with self-ensembling},
	author       = {Christian S. Perone and Pedro Ballester and Rodrigo C. Barros and Julien Cohen-Adad},
	year         = 2019,
	journal      = {NeuroImage},
	volume       = 194,
	pages        = {1--11},
	doi          = {https://doi.org/10.1016/j.neuroimage.2019.03.026},
	issn         = {1053-8119},
	url          = {https://www.sciencedirect.com/science/article/pii/S1053811919302034}
}
@inproceedings{HCS,
	title        = {Unsupervised Domain Adaptation for the Histopathological Cell Segmentation through Self-Ensembling},
	author       = {Li, Chaoqun and Zhou, Yitian and Shi, Tangqi and Wu, Yenan and Yang, Meng and Li, Zhongyu},
	year         = 2021,
	month        = {27 Sep},
	booktitle    = {Proceedings of the MICCAI Workshop on Computational Pathology},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 156,
	pages        = {151--158},
	url          = {https://proceedings.mlr.press/v156/li21a.html},
	editor       = {Atzori, Manfredo and Burlutskiy, Nikolay and Ciompi, Francesco and Li, Zhang and Minhas, Fayyaz and MÃ¼ller, Henning and Peng, Tingying and Rajpoot, Nasir and Torben-Nielsen, Ben and van der Laak, Jeroen and Veta, Mitko and Yuan, Yinyin and Zlobec, Inti},
	pdf          = {https://proceedings.mlr.press/v156/li21a/li21a.pdf}
}
@article{canis1997revised,
	title        = {Revised american society for reproductive medicine classification of endometriosis: 1996},
	author       = {Canis, M and Donnez, JG and Guzick, DS and Halme, JK and Rock, JA and Schenken, RS and Vernon, MW},
	year         = 1997,
	journal      = {Fertility and Sterility},
	publisher    = {AMER SOC REPRODUCTIVE MEDICINE 1209 MONTGOMERY HIGHWAY, BIRMINGHAM, AL 35216-2809},
	volume       = 67,
	number       = 5,
	pages        = {817--821},
	doi          = {10.1016/S0015-0282(97)81391-X}
}
@article{keckstein2020classification,
	title        = {Classification of DIE including bowel endometriosis: from r-ASRM to \#Enzian-Classification},
	author       = {Keckstein, J{\"o}rg and Hudelist, Gernot},
	year         = 2020,
	journal      = {Best Practice \& Research Clinical Obstetrics \& Gynaecology},
	publisher    = {Elsevier}
}
@article{keckstein2003enzian,
	title        = {ENZIAN-Klassifikation der tief infiltrierenden Endometriose},
	author       = {Keckstein, J and Ulrich, U and Possover, M and Schweppe, KW and others},
	year         = 2003,
	journal      = {Zentralblatt f{\"u}r Gyn{\"a}kologie},
	volume       = 125,
	pages        = 291
}
@article{liu2020ms,
	title        = {Ms-net: Multi-site network for improving prostate segmentation with heterogeneous mri data},
	author       = {Liu, Quande and Dou, Qi and Yu, Lequan and Heng, Pheng Ann},
	year         = 2020,
	journal      = {IEEE Transactions on Medical Imaging},
	publisher    = {IEEE}
}
@inproceedings{liu2020saml,
	title        = {Shape-aware Meta-learning for Generalizing Prostate MRI Segmentation to Unseen Domains},
	author       = {Liu, Quande and Dou, Qi and Heng, Pheng Ann},
	year         = 2020,
	booktitle    = {International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)},
	pages        = {475--485},
	organization = {Springer}
}
@article{TCSM-V2,
	title        = {Transformation-Consistent Self-Ensembling Model for Semisupervised Medical Image Segmentation},
	author       = {Li, Xiaomeng and Yu, Lequan and Chen, Hao and Fu, Chi-Wing and Xing, Lei and Heng, Pheng-Ann},
	year         = 2021,
	journal      = {IEEE Transactions on Neural Networks and Learning Systems},
	volume       = 32,
	number       = 2,
	pages        = {523--534},
	doi          = {10.1109/TNNLS.2020.2995319}
}
@inproceedings{Reciprocal,
	title        = {Reciprocal Learning for Semi-supervised Segmentation},
	author       = {Zeng, Xiangyun and Huang, Rian and Zhong, Yuming and Sun, Dong and Han, Chu and Lin, Di and Ni, Dong and Wang, Yi},
	year         = 2021,
	booktitle    = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2021},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {352--361},
	isbn         = {978-3-030-87196-3},
	editor       = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline}
}
@inproceedings{st++,
	title        = {St++: Make self-training work better for semi-supervised semantic segmentation},
	author       = {Yang, Lihe and Zhuo, Wei and Qi, Lei and Shi, Yinghuan and Gao, Yang},
	year         = 2022,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {4268--4277}
}
@inproceedings{CPS,
	title        = {Semi-supervised semantic segmentation with cross pseudo supervision},
	author       = {Chen, Xiaokang and Yuan, Yuhui and Zeng, Gang and Wang, Jingdong},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {2613--2622}
}
@inproceedings{DeepLabV3,
	title        = {Encoder-decoder with atrous separable convolution for semantic image segmentation},
	author       = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	year         = 2018,
	booktitle    = {Proceedings of the European conference on computer vision (ECCV)},
	pages        = {801--818}
}
@inproceedings{ResNet,
	title        = {Deep residual learning for image recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {770--778}
}
@inproceedings{LocalPhase,
	title        = {Relevance Detection in Cataract Surgery Videos by Spatio- Temporal Action Localization},
	author       = {Ghamsarian, Negin and Taschwer, Mario and Putzgruber-Adamitsch, Doris and Sarny, Stephanie and Schoeffmann, Klaus},
	year         = 2021,
	booktitle    = {2020 25th International Conference on Pattern Recognition (ICPR)},
	volume       = {},
	number       = {},
	pages        = {10720--10727},
	doi          = {10.1109/ICPR48806.2021.9412525}
}
@inproceedings{UDA-OCT,
	title        = {Unsupervised Domain Adaptation withÂ Contrastive Learning forÂ OCT Segmentation},
	author       = {Gomariz, Alvaro and Lu, Huanxiang and Li, Yun Yvonna and Albrecht, Thomas and Maunz, Andreas and Benmansour, Fethallah and Valcarcel, Alessandra M. and Luu, Jennifer and Ferrara, Daniela and Goksel, Orcun},
	year         = 2022,
	booktitle    = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2022},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {351--361},
	isbn         = {978-3-031-16452-1},
	editor       = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo}
}
@inproceedings{CL-GLF,
	title        = {Contrastive learning of global and local features for medical image segmentation with limited annotations},
	author       = {Chaitanya, Krishna and Erdil, Ertunc and Karani, Neerav and Konukoglu, Ender},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {12546--12558},
	url          = {https://proceedings.neurips.cc/paper/2020/file/949686ecef4ee20a62d16b4a2d7ccca3-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@inproceedings{cat101,
	title        = {Cataract-101: video dataset of 101 cataract surgeries},
	author       = {Schoeffmann, Klaus and Taschwer, Mario and Sarny, Stephanie and M{\"u}nzer, Bernd and Primus, Manfred J{\"u}rgen and Putzgruber, Doris},
	year         = 2018,
	booktitle    = {Proceedings of the 9th ACM multimedia systems conference},
	pages        = {421--425}
}
@article{CaDIS,
	title        = {CaDIS: Cataract dataset for surgical RGB-image segmentation},
	author       = {Grammatikopoulou, Maria and Flouty, Evangello and Kadkhodamohammadi, Abdolrahim and Quellec, Gwenol{\'e} and Chow, Andre and Nehme, Jean and Luengo, Imanol and Stoyanov, Danail},
	year         = 2021,
	journal      = {Medical Image Analysis},
	publisher    = {Elsevier},
	volume       = 71,
	pages        = 102053
}
@article{CT-free,
	title        = {Using domain knowledge for robust and generalizable deep learning-based {CT-free} {PET} attenuation and scatter correction},
	author       = {Guo, Rui and Xue, Song and Hu, Jiaxi and Sari, Hasan and Mingels, Clemens and Zeimpekis, Konstantinos and Prenosil, George and Wang, Yue and Zhang, Yu and Viscione, Marco and Sznitman, Raphael and Rominger, Axel and Li, Biao and Shi, Kuangyu},
	year         = 2022,
	month        = oct,
	journal      = {Nature Communications},
	volume       = 13,
	number       = 1,
	pages        = 5882
}
@article{test,
	title        = {The Art of Multiprocessor Programming},
	author       = {Herilhy and Shavit},
	year         = 2008,
	journal      = {IEEE Transactions on Medical Imaging},
	publisher    = {IEEE}
}
@inproceedings{chattopadhay2018grad,
	title        = {Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks},
	author       = {Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
	year         = 2018,
	booktitle    = {2018 IEEE winter conference on applications of computer vision (WACV)},
	pages        = {839--847},
	organization = {IEEE}
}
@article{sam,
	title        = {Segment Anything},
	author       = {Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr DollÃ¡r and Ross Girshick},
	year         = 2023,
	month        = 4,
	url          = {http://arxiv.org/abs/2304.02643},
	abstract     = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.}
}
@article{llama_adapter,
	title        = {Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
	author       = {Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2303.16199}
}
@article{llama,
	title        = {Llama: Open and efficient foundation language models},
	author       = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2302.13971}
}
@article{retouch,
	title        = {RETOUCH: the retinal OCT fluid detection and segmentation benchmark and challenge},
	author       = {Bogunovi{\'c}, Hrvoje and Venhuizen, Freerk and Klimscha, Sophie and Apostolopoulos, Stefanos and Bab-Hadiashar, Alireza and Bagci, Ulas and Beg, Mirza Faisal and Bekalo, Loza and Chen, Qiang and Ciller, Carlos and others},
	year         = 2019,
	journal      = {IEEE transactions on medical imaging},
	publisher    = {IEEE},
	volume       = 38,
	number       = 8,
	pages        = {1858--1874}
}
@article{mri_dataset,
	title        = {MS-Net: Multi-Site Network for Improving Prostate Segmentation With Heterogeneous MRI Data},
	author       = {Quande Liu and Qi Dou and Lequan Yu and Pheng Ann Heng},
	year         = 2020,
	month        = 9,
	journal      = {IEEE Transactions on Medical Imaging},
	volume       = 39,
	pages        = {2713--2724},
	doi          = {10.1109/TMI.2020.2974574},
	issn         = {0278-0062},
	issue        = 9
}
@article{wmh,
	title        = {Standardized Assessment of Automatic Segmentation of White Matter Hyperintensities and Results of the WMH Segmentation Challenge},
	author       = {Hugo J. Kuijf and Adria Casamitjana and D. Louis Collins and Mahsa Dadar and Achilleas Georgiou and others},
	year         = 2019,
	month        = 11,
	journal      = {IEEE Transactions on Medical Imaging},
	volume       = 38,
	pages        = {2556--2568},
	doi          = {10.1109/TMI.2019.2905770},
	issn         = {0278-0062},
	issue        = 11
}
@inproceedings{hu2022lora,
	title        = {Lo{RA}: Low-Rank Adaptation of Large Language Models},
	author       = {Edward J Hu and Yelong Shen and Phillip Wallis and others},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations}
}
@article{wu2023medical,
	title        = {Medical sam adapter: Adapting segment anything model for medical image segmentation},
	author       = {Wu, Junde and Fu, Rao and Fang, Huihui and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2304.12620}
}
@inproceedings{milletari2016dice,
	title        = {V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation},
	author       = {F. Milletari and N. Navab and S. Ahmadi},
	year         = 2016,
	month        = {oct},
	booktitle    = {2016 Fourth International Conference on 3D Vision (3DV)},
	publisher    = {IEEE Computer Society},
	address      = {Los Alamitos, CA, USA},
	volume       = {},
	pages        = {565--571},
	doi          = {10.1109/3DV.2016.79},
	issn         = {},
	abstract     = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
	keywords     = {image segmentation;feature extraction;biomedical imaging;three-dimensional displays;neural networks;magnetic resonance imaging;two dimensional displays}
}
@inproceedings{wang2021tent,
	title        = {Tent: Fully Test-Time Adaptation by Entropy Minimization},
	author       = {Wang, Dequan and Shelhamer, Evan and Liu, Shaoteng and others},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations}
}
@article{zhang2023improving,
	title        = {Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation},
	author       = {Zhang, Haojie and Su, Yongyi and Xu, Xun and Jia, Kui},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2312.03502}
}
@inproceedings{lin2017focal,
	title        = {Focal loss for dense object detection},
	author       = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {2980--2988}
}
@article{loshchilov2017decoupled,
	title        = {Decoupled weight decay regularization},
	author       = {Loshchilov, Ilya and Hutter, Frank},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1711.05101}
}
@article{MedSAM,
	title        = {Segment Anything in Medical Images},
	author       = {Ma, Jun and He, Yuting and Li, Feifei and Han, Lin and You, Chenyu and Wang, Bo},
	year         = 2024,
	journal      = {Nature Communications},
	volume       = 15,
	pages        = {1--9}
}
@inproceedings{dosovitskiy2021an,
	title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	author       = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and others},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations}
}
@article{xu2023parameter,
	title        = {Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment},
	author       = {Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2312.12148}
}
@inproceedings{houlsby2019parameter,
	title        = {Parameter-efficient transfer learning for NLP},
	author       = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2790--2799},
	organization = {PMLR}
}
@article{pfeiffer2020adapterfusion,
	title        = {AdapterFusion: Non-destructive task composition for transfer learning},
	author       = {Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and others},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2005.00247}
}
@article{mahabadi2021parameter,
	title        = {Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks},
	author       = {Mahabadi, Rabeeh Karimi and Ruder, Sebastian and others},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2106.04489}
}
@article{lin2020exploring,
	title        = {Exploring versatile generative language model via parameter-efficient transfer learning},
	author       = {Lin, Zhaojiang and Madotto, Andrea and Fung, Pascale},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2004.03829}
}
@article{chen2022vision,
	title        = {Vision transformer adapter for dense predictions},
	author       = {Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.08534}
}
@inproceedings{chen2023sam,
	title        = {Sam-adapter: Adapting segment anything in underperformed scenes},
	author       = {Chen, Tianrun and Zhu, Lanyun and others},
	year         = 2023,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {3367--3375}
}
@article{zhang2023customized,
	title        = {Customized segment anything model for medical image segmentation},
	author       = {Zhang, Kaidong and Liu, Dong},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2304.13785}
}
@article{lester2021power,
	title        = {The power of scale for parameter-efficient prompt tuning},
	author       = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2104.08691}
}
@article{ke2024segment,
	title        = {Segment anything in high quality},
	author       = {Ke, Lei and Ye, Mingqiao and Danelljan, Martin and Tai, Yu-Wing and Tang, Chi-Keung and others},
	year         = 2024,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 36
}
@article{zhang2023comprehensive,
	title        = {A Comprehensive Survey on Segment Anything Model for Vision and Beyond},
	author       = {Zhang, Chunhui and Liu, Li and Cui, Yawen and Huang, Guanjie and Lin, Weilin and Yang, Yiqian and Hu, Yuehong},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2305.08196}
}
@article{janouskova2023single,
	title        = {Single Image Test-Time Adaptation for Segmentation},
	author       = {Janouskova, Klara and Shor, Tamir and Baskin, Chaim and Matas, Jiri},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2309.14052}
}
@inproceedings{wang2023dynamically,
	title        = {Dynamically Instance-Guided Adaptation: A Backward-Free Approach for Test-Time Domain Adaptive Semantic Segmentation},
	author       = {Wang, Wei and Zhong, Zhun and Wang, Weijie and Chen, Xi and Ling, Charles and Wang, Boyu and Sebe, Nicu},
	year         = 2023,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {24090--24099}
}
@inproceedings{ghamsarian2023domain,
	title        = {Domain adaptation for medical image segmentation using transformation-invariant self-training},
	author       = {Ghamsarian, Negin and Gamazo Tejero, Javier and M{\'a}rquez-Neila, Pablo and Wolf, Sebastian and Zinkernagel, Martin and Schoeffmann, Klaus and Sznitman, Raphael},
	year         = 2023,
	booktitle    = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
	pages        = {331--341},
	organization = {Springer}
}
@inproceedings{hoyer2023mic,
	title        = {MIC: Masked image consistency for context-enhanced domain adaptation},
	author       = {Hoyer, Lukas and Dai, Dengxin and Wang, Haoran and Van Gool, Luc},
	year         = 2023,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {11721--11732}
}
@inproceedings{chen2023pipa,
	title        = {Pipa: Pixel-and patch-wise self-supervised learning for domain adaptative semantic segmentation},
	author       = {Chen, Mu and Zheng, Zhedong and Yang, Yi and Chua, Tat-Seng},
	year         = 2023,
	booktitle    = {Proceedings of the 31st ACM International Conference on Multimedia},
	pages        = {1905--1914}
}
@inproceedings{kundu2020universal,
	title        = {Universal source-free domain adaptation},
	author       = {Kundu, Jogendra Nath and Venkat, Naveen and Babu, R Venkatesh and others},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {4544--4553}
}
@inproceedings{xia2021adaptive,
	title        = {Adaptive adversarial network for source-free domain adaptation},
	author       = {Xia, Haifeng and Zhao, Handong and Ding, Zhengming},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {9010--9019}
}
@article{oquab2023dinov2,
	title        = {Dinov2: Learning robust visual features without supervision},
	author       = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2304.07193}
}
@inproceedings{coift_liew2021deep,
	title        = {Deep interactive thin object selection},
	author       = {Liew, Jun Hao and Cohen, Scott and Price, Brian and Mai, Long and Feng, Jiashi},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
	pages        = {305--314}
}
@inproceedings{hrsod_zeng2019towards,
	title        = {Towards high-resolution salient object detection},
	author       = {Zeng, Yi and Zhang, Pingping and Zhang, Jianming and Lin, Zhe and Lu, Huchuan},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF international conference on computer vision},
	pages        = {7234--7243}
}
@inproceedings{qin2022highly,
	title        = {Highly accurate dichotomous image segmentation},
	author       = {Qin, Xuebin and Dai, Hang and Hu, Xiaobin and Fan, Deng-Ping and Shao, Ling and Van Gool, Luc},
	year         = 2022,
	booktitle    = {European Conference on Computer Vision},
	pages        = {38--56},
	organization = {Springer}
}
@inproceedings{fss_li2020fss,
	title        = {Fss-1000: A 1000-class dataset for few-shot segmentation},
	author       = {Li, Xiang and Wei, Tianhan and Chen, Yau Pun and Tai, Yu-Wing and Tang, Chi-Keung},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {2869--2878}
}
@article{eccsd_shi2015hierarchical,
	title        = {Hierarchical image saliency detection on extended CSSD},
	author       = {Shi, Jianping and Yan, Qiong and Xu, Li and Jia, Jiaya},
	year         = 2015,
	journal      = {IEEE transactions on pattern analysis and machine intelligence},
	publisher    = {IEEE},
	volume       = 38,
	number       = 4,
	pages        = {717--729}
}
@article{msra_cheng2014global,
	title        = {Global contrast based salient region detection},
	author       = {Cheng, Ming-Ming and Mitra, Niloy J and Huang, Xiaolei and Torr, Philip HS and Hu, Shi-Min},
	year         = 2014,
	journal      = {IEEE transactions on pattern analysis and machine intelligence},
	publisher    = {Ieee},
	volume       = 37,
	number       = 3,
	pages        = {569--582}
}
@inproceedings{dutomron_yang2013saliency,
	title        = {Saliency detection via graph-based manifold ranking},
	author       = {Yang, Chuan and Zhang, Lihe and Lu, Huchuan and Ruan, Xiang and Yang, Ming-Hsuan},
	year         = 2013,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {3166--3173}
}
@article{li2021prefix,
	title        = {Prefix-tuning: Optimizing continuous prompts for generation},
	author       = {Li, Xiang Lisa and Liang, Percy},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2101.00190}
}
@article{liu2023gpt,
	title        = {GPT understands, too},
	author       = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
	year         = 2023,
	journal      = {AI Open},
	publisher    = {Elsevier}
}
@article{he2021towards,
	title        = {Towards a unified view of parameter-efficient transfer learning},
	author       = {He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2110.04366}
}
@article{zou2024segment,
	title        = {Segment everything everywhere all at once},
	author       = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
	year         = 2024,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 36
}
@article{abbott2024neural,
	title        = {Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures},
	author       = {Abbott, Vincent},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2402.05424}
}
@InProceedings{Tejero_2023_CVPR,
    author    = {Gamazo Tejero, Javier and Zinkernagel, Martin S. and Wolf, Sebastian and Sznitman, Raphael and M\'arquez-Neila, Pablo},
    title     = {Full or Weak Annotations? An Adaptive Strategy for Budget-Constrained Annotation Campaigns},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {11381-11391}
}
@article{tejero2023predicting,
  title={Predicting OCT biological marker localization from weak annotations},
  author={Gamazo Tejero, Javier and M{\'a}rquez Neila, Pablo and Kurmann, Thomas and Gallardo, Mathias and Zinkernagel, Martin and Wolf, Sebastian and Sznitman, Raphael},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={19667},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{moore1998cramming,
  title={Cramming more components onto integrated circuits},
  author={Moore, Gordon E},
  journal={Proceedings of the IEEE},
  volume={86},
  number={1},
  pages={82--85},
  year={1998},
  publisher={Ieee}
}
@article{koomey2010implications,
  title={Implications of historical trends in the electrical efficiency of computing},
  author={Koomey, Jonathan and Berard, Stephen and Sanchez, Marla and Wong, Henry},
  journal={IEEE Annals of the History of Computing},
  volume={33},
  number={3},
  pages={46--54},
  year={2010},
  publisher={IEEE}
}
@inproceedings{steinkraus2005using,
  title={Using GPUs for machine learning algorithms},
  author={Steinkraus, Dave and Buck, Ian and Simard, Patrice Y},
  booktitle={Eighth International Conference on Document Analysis and Recognition (ICDAR'05)},
  pages={1115--1120},
  year={2005},
  organization={IEEE}
}
@inproceedings{chellapilla2006high,
  title={High performance convolutional neural networks for document processing},
  author={Chellapilla, Kumar and Puri, Sidd and Simard, Patrice},
  booktitle={Tenth international workshop on frontiers in handwriting recognition},
  year={2006},
  organization={Suvisoft}
}
@inproceedings{raina2009large,
  title={Large-scale deep unsupervised learning using graphics processors},
  author={Raina, Rajat and Madhavan, Anand and Ng, Andrew Y},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={873--880},
  year={2009}
}
@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}
@misc{epoch2023pcdtrends,
  title = "Parameter, Compute and Data Trends in Machine Learning",
  author = {Epoch},
  year = 2024,
  url = {https://epochai.org/data/epochdb/visualization},
  note = "Accessed: 2024-04-16"
}
@inproceedings{sevilla2022compute,
  title={Compute trends across three eras of machine learning},
  author={Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}
@article{taesiri2024imagenet,
  title={Imagenet-hard: The hardest images remaining from a study of the power of zoom and spatial biases in image classification},
  author={Taesiri, Mohammad Reza and Nguyen, Giang and Habchi, Sarra and Bezemer, Cor-Paul and Nguyen, Anh},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{bozinovski1976influence,
  title={The influence of pattern similarity and transfer learning upon training of a base perceptron b2},
  author={Bozinovski, Stevo and Fulgosi, Ante},
  booktitle={Proceedings of Symposium Informatica},
  volume={3},
  pages={121--126},
  year={1976}
}
@article{time2023chatgpt,
 author  = {Perrigo, Billy},
 date    = {2023-01-18},
 title   = {OpenAI Used Kenyan Workers on Less Than \$2 Per Hour to Make ChatGPT Less Toxic},
 journal = {TIME Magazine},
 url     = {https://time.com/6247678/openai-chatgpt-kenya-workers/},
 urldate = {2024-04-26}
}
@article{googlecompute,
    author = {Google},
    title = {AI Platform Data Labeling Service pricing},
    urldate = {2024-04-29},
    url = {https://cloud.google.com/ai-platform/data-labeling/pricing},
    year = {n.d.}
}
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@article{nvidia2023gpuperformance,
 author  = {Merritt, Rick},
 date    = {2023-08-29},
 title   = {Wide Horizons: NVIDIA Keynote Points Way to Further AI Advances},
 journal = {NVIDIA Blog},
 url     = {https://blogs.nvidia.com/blog/hot-chips-dally-research/},
 urldate = {2024-05-07}
}
@article{verge2023chatgpt,
    author = {Porter, Jon},
    title = {ChatGPT continues to be one of the fastest-growing services ever},
    journal = {The Verge},
    year = {2023},
    URL = {https://www.theverge.com/2023/11/6/23948386/chatgpt-active-user-count-openai-developer-conference}
}
@article{luccioni2023estimating,
  title={Estimating the carbon footprint of bloom, a 176B parameter language model},
  author={Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={253},
  pages={1--15},
  year={2023}
}
@article{sutton2019bitter,
  title={The bitter lesson},
  author={Sutton, Richard},
  journal={Incomplete Ideas (blog)},
  volume={13},
  number={1},
  pages={38},
  year={2019}
}
@book{sejnowski2018deep,
  title={The deep learning revolution},
  author={Sejnowski, Terrence J},
  year={2018},
  publisher={MIT press}
}
@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}
@article{dubey2022activation,
  title={Activation functions in deep learning: A comprehensive survey and benchmark},
  author={Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
  journal={Neurocomputing},
  volume={503},
  pages={92--108},
  year={2022},
  publisher={Elsevier}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}
@book{bellman1957dynamic,
  title={Dynamic Programming},
  author={Bellman, R. and Rand Corporation and Karreman Mathematics Research Collection},
  isbn={9780691079516},
  lccn={57005444},
  series={Rand Corporation research study},
  url={https://books.google.ch/books?id=wdtoPwAACAAJ},
  year={1957},
  publisher={Princeton University Press}
}
@book{mitchell1997machine,
  author = {Mitchell, Tom M},
  biburl = {https://www.bibsonomy.org/bibtex/2ea9f893d9d19c182bcf2822eb590fe4f/msteininger},
  interhash = {479a66c32badb3a455fbdcf8e6633a5d},
  intrahash = {ea9f893d9d19c182bcf2822eb590fe4f},
  keywords = {book ml},
  number = 9,
  publisher = {McGraw-hill New York},
  timestamp = {2022-09-08T17:32:10.000+0200},
  title = {Machine learning},
  volume = 1,
  year = 1997
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@article{jaccard1902lois,
  title={Lois de distribution florale dans la zone alpine},
  author={Jaccard, Paul},
  journal={Bull Soc Vaudoise Sci Nat},
  volume={38},
  pages={69--130},
  year={1902}
}
@article{dice1945measures,
  title={Measures of the amount of ecologic association between species},
  author={Dice, Lee R},
  journal={Ecology},
  volume={26},
  number={3},
  pages={297--302},
  year={1945},
  publisher={JSTOR}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}
@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"
}
@article{supercomputers2023dongarra,
    author = {Dongarra, Jon and Meuer, Martin and Simon, Horst and Strohmaier, Eric},
    title = {TOP500, Performance Development},
    journal = {TOP 500},
    year = {2023},
    URL = {https://top500.org/lists/top500/2023/11/}
}
@misc{attention_slides,
  author={Fleuret, FranÃ§ois},
  title = {Deep Learning / 13.1. Attention for Memory and Sequence Translation},
  howpublished = {\url{https://fleuret.org/dlc/materials/dlc-handout-13-1-attention-memory-translation.pdf}},
  year = {2020},
  note = {Accessed: {2024â0-06}}
}
@article{graves2014neural,
  title={Neural turing machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}
@article{ramachandran2019stand,
  title={Stand-alone self-attention in vision models},
  author={Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{isensee2024nnu,
  title={nnU-Net Revisited: A Call for Rigorous Validation in 3D Medical Image Segmentation},
  author={Isensee, Fabian and Wald, Tassilo and Ulrich, Constantin and Baumgartner, Michael and Roy, Saikat and Maier-Hein, Klaus and Jaeger, Paul F},
  journal={arXiv preprint arXiv:2404.09556},
  year={2024}
}
@article{krige1951statistical,
  title={A statistical approach to some basic mine valuation problems on the Witwatersrand},
  author={Krige, Daniel G},
  journal={Journal of the Southern African Institute of Mining and Metallurgy},
  volume={52},
  number={6},
  pages={119--139},
  year={1951},
  publisher={Southern African Institute of Mining and Metallurgy}
}
@book{matheron1962traite,
  title={Trait{\'e} de g{\'e}ostatistique appliqu{\'e}e},
  author={Matheron, Georges},
  number={14},
  year={1962},
  publisher={Editions Technip}
}
@article{lorenzi2019probabilistic,
  title={Probabilistic disease progression modeling to characterize diagnostic uncertainty: application to staging and prediction in Alzheimer's disease},
  author={Lorenzi, Marco and Filippone, Maurizio and Frisoni, Giovanni B and Alexander, Daniel C and Ourselin, S{\'e}bastien and Alzheimer's Disease Neuroimaging Initiative and others},
  journal={NeuroImage},
  volume={190},
  pages={56--68},
  year={2019},
  publisher={Elsevier}
}
@article{gonzalez2007creating,
  title={Creating low-cost soil maps for tropical agriculture using gaussian processes},
  author={Gonzalez, Juan Pablo and Cook, Simon E and Oberth{\"u}r, Thomas and Jarvis, Andy and Bagnell, J Andrew and Dias, M Bernardine},
  year={2007},
  publisher={Information and Communication and Technologies for Development (ICTD)}
}
@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  number={4},
  year={2006},
  publisher={Springer}
}